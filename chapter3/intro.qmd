# Introduction {#sec-ch3-intro}

Recurrent Neural Networks (RNNs) [@rumelhart1986learning], and their variants, Long Short-Term Memory (LSTMs), are widely used for modeling sequence data because of their ability to capture both short- and long-term dependencies. In natural language processing, they have been successfully applied to tasks such as handwriting recognition [@graves2008novel], language modeling [@mikolov2012statistical], speech recognition [@chan2015speech; @chiu2017speech], and machine translation [@sutskever2014neural; @bahdanau2014neural]. Beyond language, RNNs and LSTMs have also shown effective in complex time series forecasting and have been employed for applications including financial market prediction [@siami2019comparing; @muncharaz2020comparing; @pirani2022comparing], energy forecasting [@manero2018energy; @sandhu2019energy; @yu2019energy; @paramasivan2021energy], weather and climate modeling [@salman2018weather; @haq2022climate], and epidemiological trend analysis [@chimmula2020disease; @wang2020disease]. 

However, previous studies comparing LSTMs to traditional models often claim LSTM superiority, a conclusion that can be misleading when the benchmarks chosen are inappropriate. For example, LSTMs are frequently compared to autoregressive integrated moving average (ARIMA) models, even when the assumptions underlying ARIMA models such as stationarity and normally distributed errors are not satisfied [@hewamalage2023critique]. An early survey also reported more nuanced results, showing that RNNs including LSTMs outperform classical benchmarks on some datasets and metrics, but not consistently [@hewamalage2021reviewrnn]. Similar concerns have also been raised in discussions of newer foundation-model approaches [@bergmeir2024critiquellms]. These observations highlight the need to evaluate deep learning models against more flexible probabilistic alternatives. 

In line with this, both probabilistic and deep learning models have been shown to be effective for forecasting univariate time series. For example, a prior study [@hassan2021deep] demonstrated that the probabilistic MTD model and the deep learning LSTM network achieved similar predictive accuracy in modeling disease spread, with both slightly outperforming classical ARIMA models. These findings suggest that both probabilistic and deep learning approaches hold promise, yet their relative strengths under varying data conditions remain underexplored in the univariate setting within the statistics and machine learning (ML) literature. 

To address the concerns about benchmark limitations and to investigate the relative performance of probabilistic and deep learning models, we conduct a rigorous comparison of LSTM and MTD models in the univariate setting. Unlike prior work that benchmarks LSTMs primarily against mis-specified linear models such as ARIMA, we evaluate LSTM against a probabilistic alternative that does not require restrictive assumptions and is better suited to non-linear, non-Gaussian dynamics. Our controlled simulations focus on stationary but non-Gaussian data-generating processes, systematically varying conditions such as marginal skewness, dependency structure, and zero-inflation to assess each modelâ€™s strengths and weaknesses. We then complement these simulations with a real-world data application to provide a grounded assessment of practical forecasting performance. 

The rest of the chapter is organized as follows. We review RNN and LSTM architectures and their foundational concepts, and provide an overview of hyperparameter tuning, training, and evaluation metrics in @sec-ch3-background. Simulation results comparing MTDs and LSTMs are presented in @sec-ch3-simu, followed by results from the real-world data application in @sec-ch3-real. Finally, we conclude with a discussion in @sec-ch3-discussion. 

