[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#motivation-and-objective",
    "href": "index.html#motivation-and-objective",
    "title": "Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks",
    "section": "1.1 Motivation and Objective",
    "text": "1.1 Motivation and Objective\nModeling complex patterns in sequence data is a central challenge across domains such as energy, insurance, and transportation. However, real-world time series often exhibit characteristics such as skewness and zero inflation. If left unaddressed, these features can undermine modeling and prediction. For instance, wind speeds are typically skewed, while medical expenditures, insurance claims, and transportation safety measures may include an excess of zeros. These features motivate the development of flexible models capable of capturing both continuous skewed behavior and semicontinuous, zero-inflated structures. Figure 1.1 and Figure 1.2 illustrate typical skewed and zero-inflated time series that motivate these modeling choices.\nRecent advances in artificial intelligence (AI) have introduced powerful deep learning architectures, particularly Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. These models effectively capture nonlinear and long-range dependence from data, offering fast predictions without explicit probabilistic assumptions. However, claims of LSTM superiority can be misleading when compared against inappropriate benchmarks such as ARIMA models, and a prior work has found comparable predictive performance between probabilistic Mixture Transition Distribution (MTD) model and deep learning LSTM network for disease spread. These considerations motivate a comparison of deep learning approaches with probabilistic models that are flexible enough to handle nonlinear, non-Gaussian, and zero-inflated dynamics, providing a more realistic benchmark than traditional methods.\nAt the same time, classical statistical approaches remain indispensable, especially when robustness, interpretability, and uncertainty quantification are priorities. The Gamma Mixture Transition Distribution (Gamma MTD) model and its zero-inflated extension (ZIGamma MTD) provide flexible, interpretable frameworks for skewed and zero-inflated time series. Using a copula-based formulation, these models extend the traditional MTD framework, enabling accurate modeling of continuous and semicontinuous data commonly observed in real-world applications.\nThis dissertation develops the Gamma MTD and ZIGamma MTD models, evaluates their predictive performance and robustness, and compares them with modern deep learning approaches such as LSTMs. By integrating insights from both probabilistic and AI-driven frameworks, this work highlights their complementary strengths, limitations, and practical applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#data-challenges",
    "href": "index.html#data-challenges",
    "title": "Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks",
    "section": "1.2 Data Challenges",
    "text": "1.2 Data Challenges\nTo account for skewness and zero inflation in real-world time series, we develop the copula-based Gamma MTD model along with its zero-inflated extension, ZIGamma MTD, offering a robust, flexible, and interpretable framework for modeling such data.\nGamma MTD.\nZIGamma MTD.\n\n\n\n\n\n\nFigure 1.1: Time series plot of observed wind speeds (m/s) at heights of (a) 50 m, (b) 10 m, and (c) 2 m above ground level at the Limon Wind Energy Center, Colorado, for the year 2024. Data sourced from MERRA-2 via the NASA GES DISC Earthdata API.\n\n\n\n\n\n\n\n\n\nFigure 1.2: Time series plot of observed values for varying zero-inflated probability (a) \\(P = 0.1\\), (b) \\(P = 0.5\\) , and (c) \\(P = 0.7\\). Data generated from a ZIGamma MTD model with mean, scale, and threshold parameters \\(\\mu = 7\\), \\(\\beta = 1\\), and \\(\\epsilon = 0.1\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#modeling-approaches",
    "href": "index.html#modeling-approaches",
    "title": "Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks",
    "section": "1.3 Modeling Approaches",
    "text": "1.3 Modeling Approaches\nTo ensure a fair and balanced evaluation, we systematically examine two complementary modeling approaches: probabilistic MTD models and deep learning methods, such as LSTM networks.\nMTD models represent each conditional distribution as a mixture of \\(l\\) transition kernels, weighting past lags probabilistically to capture complex temporal dependence. The models are particularly effective for non-Gaussian, skewed, and zero-inflated data. The copula-based formulation further enhances modeling of continuous and semicontinuous patterns.\nRecurrent Neural Network (RNN) architectures, including LSTMs, model temporal dependence by propagating recurrent units over time. These networks learn internal states directly from data using backpropagation through time (BPTT) without relying on explicit probabilistic assumptions. LSTMs capture long-range nonlinear dependence efficiently, making them well-suited for fast prediction on large-scale datasets.\nAlthough both MTD models and LSTMs describe evolving dynamics, they differ fundamentally. MTD models employ probabilistic transition mechanisms, whereas LSTMs rely on deterministic, learned transformations of hidden and cell states. This distinction has practical implications: MTD models tend to provide higher predictive accuracy, improved robustness, and greater interpretability, but at greater computational cost and more challenging model design. LSTMs provide faster predictions with lower accuracy and are easier to deploy, though less interpretable.\nBy combining insights from both classical and AI-driven frameworks, this dissertation aims to provide practical guidance for modeling complex, skewed, and zero-inflated time series data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#outline-of-the-dissertation-chapters",
    "href": "index.html#outline-of-the-dissertation-chapters",
    "title": "Copula-Based Mixture Transition Distribution Models for Forecasting Skewed and Zero-Inflated Time Series: Methodology and Comparisons with Deep Learning LSTM Networks",
    "section": "1.4 Outline of the Dissertation Chapters",
    "text": "1.4 Outline of the Dissertation Chapters\nThe rest of the dissertation is organized as follows. In Part I, we propose the copula-based Gamma MTD model, detailing its design, estimation, and prediction. While the focus is on the Gamma time series, the framework is readily generalizable to other marginal distributions, such as the lognormal. In Part II, we extend the Gamma MTD model to accommodate zero-inflated time series, resulting in the ZIGamma MTD model. This extension again demonstrates the framework’s generalizability to alternative marginal distributions beyond the Gamma. In both models, we use a Gaussian copula to capture dependence, although alternative copula families, such as Clayton and Gumbel, can also be adopted to better model tail dependence and asymmetry. In Part III, we compare the MTD models with deep learning approaches, specifically the Long Short-Term Memory (LSTM) networks, evaluating predictive performance and robustness through simulation studies and real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1/intro.html",
    "href": "chapter1/intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "Time series data consist of observations measured at regular time intervals. In these data types, observations often exhibit temporal dependence; that is, observations from recent time lags tend to be similar. Examples of time series data are sensor readings, stock prices, sale figures, energy production, weather data, and various other metrics.\nTime series models capture how past values contribute to the current value and use this information to predict future values. In the autoregressive (AR) model with order \\(p\\), for example, each current value depends on all \\(p\\) past values, with fixed, deterministic weights. The mixture transition distribution (MTD) model extends the AR models to accommodate discrete, continuous, and mixed time series, expanding its range of applications. The MTD model models each conditional distribution as a mixture of transition kernels, with random, stochastic weights.\nThe first MTD model was developed in 1985 to model high-order Markov chains (Raftery 1985a, 1985b), followed by several variant models over the years. Our work builds upon the architecture of the MTD model introduced in 2022 by Zheng, Kottas, and Sansó (2022). This model includes various applications, such as the Gaussian MTD, Poisson MTD, negative binomial MTD, and Lomax MTD regression models, extending beyond linear, Gaussian dynamics. However, there are two limitations. First, under this model framework, the transition kernel lacks component-varying parameters. Second, for certain invariant marginal distributions, the transition kernel may either require careful construction or can result in a form that is not explicitly defined or too complex.\nWe propose to incorporate copulas into the transition kernels to address the second limitation. Using copulas, dependence structures and marginal distributions can be modeled separately, enabling a choice of copula families that effectively capture data’s dependence while allowing flexibility in marginal selection. The proposed copula-based MTD model enables flexible dependence modeling and accommodates any continuous marginals, thereby enhancing modeling capabilities and flexibility.\nThe rest of the chapter is organized as follows. We review the MTD model developed by Zheng, Kottas, and Sansó (2022) in Chapter 3. We present the proposed model in Chapter 4 and provide an overview of the MCMC algorithm for parameter estimation in Chapter 5. We present the results of various simulations conducted to assess the accuracy and performance of the proposed model in Chapter 6 and discuss the model’s predictive capabilities, including uncertainty quantification, in Chapter 7. Finally, we conclude with a discussion in Chapter 8. Appendix G provides the instruction for installing the extended mtd package.\n\n\n\n\nRaftery, Adrian. 1985a. “A Model for High-Order Markov Chains.” Journal of the Royal Statistical Society Series B: Statistical Methodology 47 (3): 528–39.\n\n\n———. 1985b. “A New Model for Discrete-Valued Time Series: Autocorrelations and Extensions.” Rassegna Di Metodi Statistici Ed Applicazioni 3 (4): 149–62.\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On Construction and Estimation of Stationary Mixture Transition Distribution Models.” Journal of Computational and Graphical Statistics 31 (1): 283–93.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1/background.html",
    "href": "chapter1/background.html",
    "title": "3  Background",
    "section": "",
    "text": "3.1 Mixture Transition Distribution Models for Time Series\nThe MTD model is a state space model. The MTD model was initially developed in 1985 to model high-order Markov chains (Raftery 1985a, 1985b). On a finite state space, the model offers a parsimonious approximation of higher-order Markov chains. On a more general state space, the model can capture non-Gaussian and nonlinear features, such as flat stretches, bursts, outliers, and change points (Raftery 1994; Le, Martin, and Raftery 1996). The class of MTD models and their generalizations have diverse applications, including wind power forecasting, social behavior analysis, DNA sequence modeling, and forecasting of stock prices and inflation rates. A complete review of the MTD model and its applications can be found in Berchtold and Raftery (2002). Recent applications extend to the modeling of crime incidents and precipitation patterns Zheng, Kottas, and Sansó (2022) and the network analysis of financial markets D’Amico, De Blasis, and Petroni (2023). Although the MTD model has primarily been used for modeling time series, its generalizations has also demonstrated success in modeling spatial data. Some applications within the spatial context can be found in Berchtold (2001), Zheng, Kottas, and Sansó (2023b), and Zheng, Kottas, and Sansó (2023a).\nIn the general MTD model framework, the joint data distribution over a directed acyclic graph (DAG) is modeled as a weighted combination of first-order component densities or transition kernels. A DAG simplifies complex relationships in the data, enabling flexible and parsimonious multivariate non-Gaussian modeling. Let \\(\\{ X_t: t \\in \\mathbb{N} \\}\\) be a time series, represented by a sequence of random variables in an arbitrary state space and \\(f(\\textbf{x})\\), where \\(\\textbf{x} = {(X_1,..., X_t)}^T\\), be the joint distribution of \\(X_1\\) to \\(X_t\\). By applying the chain rule of probability theory and constructing the model on a DAG, the joint distribution of \\(X_1\\) to \\(X_t\\) can be factorized into a product of conditional distributions as\n\\(f(x_t | \\textbf{x}^{t-1})\\) is the conditional probability density function (pdf) of current value \\(X_t\\) given all of its past values \\(\\textbf{X}^{t-1} = \\textbf{x}^{t-1}\\), where \\(\\textbf{X}^{t-1} = \\{X_i: i \\leq t - 1 \\}\\) and \\(\\textbf{x}^{t-1} = \\{x_i: i \\leq t - 1 \\}\\). The joint distribution in \\(\\eqref{eq:joint_distribution}\\) corresponds to a directed graphical model (Jordan (2004); also known as a Bayesian network), where the conditional independence structure among the random variables is encoded by a DAG. Figure \\(\\ref{Linear}\\) and \\(\\ref{DAG}\\) provide visual illustrations of the linear and directed relationships between \\(X_1\\) through \\(X_t\\) on a DAG, respectively. In Figure \\(\\ref{DAG}\\), let \\(X_1, ..., X_t\\) be vertices or nodes in a graph. The set \\(\\textbf{X}^{t-1}\\), which consists of nodes that have directed edges pointed to \\(X_t\\), is called the parent or conditioning set of \\(X_t\\).\nAs \\(t\\) increases, the size of the conditioning set of \\(X_t\\) can become notably large. Zheng, Kottas, and Sansó (2022) address the challenge of modeling a non-Gaussian conditional density with a high-dimensional conditioning set using the structured mixture model. When the order of the MTD model is set to \\(L \\ll t\\), the conditioning set of \\(x_t\\) is reduced to \\(\\{x_{t-L}, ..., x_{t-1} \\} \\subset \\{x_1, ..., x_{t-1}\\}\\). Consequently, in Figure \\(\\ref{DAG}\\), the number of directed edges pointing to each \\(X_t\\) is reduced to at most \\(L\\). Each current value in the MTD model references only to the \\(L\\) lagged values, rather than the entire history of the time series.\nEach conditional in \\(\\eqref{eq:joint_distribution}\\) is modeled as a mixture of \\(L\\) transition kernels, with mixture weights. Transition kernels, similar to transition probabilities in discrete-valued time series, describe how a probability distribution moves from one state to another in a stochastic process, but applied to continuous or more general state spaces. Transition kernels represent the influence of the \\(l\\)th lag value on the current value. Mixture weights indicate the contribution of that influence. Let \\(\\{ X_t: t \\in \\mathbb{N} \\}\\) be a time series, represented by a sequence of random variables in an arbitrary state space. For \\(t &gt; L\\), the MTD model specifies the conditional distribution of \\(X_t\\) given \\(\\textbf{X}^{t-1} = \\textbf{x}^{t-1}\\) as\n\\(f_l (x_t | x_{t-l})\\) is the conditional pdf of \\(X_t\\) with respect to the \\(l\\)th transition kernel given that \\({X}_{t-l} = {x}_{t-l}\\). \\(w_l\\) are weight parameters, where \\(w_l \\geq 0\\) such that \\(\\sum_{l=1}^L w_l = 1\\). Transition kernels in \\(\\eqref{eq:cond_distribution}\\) capture dependence between the current value and its lag values. Weight parameters assign specific weights to the transition kernels, determining the relative contribution of the lagged values’ influence. We will expand the discussion of transition kernels and mixture weights in the next section.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter1/background.html#mixturetransitiondistributionmodels",
    "href": "chapter1/background.html#mixturetransitiondistributionmodels",
    "title": "3  Background",
    "section": "",
    "text": "\\[\\begin{equation}\nf(\\textbf{x}) = f(x_1) \\prod_{t=2}^t f(x_t | \\textbf{x}^{t-1}).\n\\label{eq:joint_distribution}\n\\end{equation}\\]\n\n\n\n\\[\\begin{equation}\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l f_l (x_t | x_{t-l}).\n\\label{eq:cond_distribution}\n\\end{equation}\\]",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter1/background.html#modelconstruction",
    "href": "chapter1/background.html#modelconstruction",
    "title": "3  Background",
    "section": "3.2 Model Construction",
    "text": "3.2 Model Construction\nEarlier MTD models were built using frequentist approaches. Estimation and prediction in the MTD model by Zheng, Kottas, and Sansó (2022) is constructed with a focus on Bayesian methodologies.\n\n3.2.1 Mixture Weights\nZheng, Kottas, and Sansó (2022) consider three weight types: weights with a uniform Dirichlet prior, a truncated version of the stick-breaking prior, and the cdf-based Dirichlet process prior. We use the weight with the cdf-based prior, which will be the focus of our discussion.\nLet \\(x_k \\in [0, 1]\\) with \\(\\sum_{k=1}^K x_k = 1\\). The Dirichlet distribution, denoted as \\(Dirichlet(\\boldsymbol{\\alpha})\\), is a discrete distribution on \\([0, 1]^K\\). It is characterized by a vector of concentration parameters \\(\\boldsymbol{\\alpha}\\), where \\(\\alpha_k &gt; 0\\), and the mean of \\(x_k\\) is \\(\\alpha_k / \\sum_{k=1}^K \\alpha_k\\), for \\(k=1,...,K\\). The Dirichlet process, denoted as \\(DP(\\alpha, \\boldsymbol{G})\\), is a discrete distribution over probability distributions. The Dirichlet process generates a distribution that shrinks around a base distribution \\(\\boldsymbol{G}\\), with the degree of shrinkage controlled by a concentration parameter \\(\\alpha\\). The base distribution determines the form of the distribution generated by the Dirichlet process, and the concentration parameter controls the degree of shrinkage of the resulting distribution toward the base distribution.\nThe weight associated with a cdf-based Dirichlet process prior assumes that the weights are increments of a cdf \\(G\\). This prior is denoted as \\(CDP(\\cdot | \\boldsymbol{1}_L / L)\\), where \\(\\boldsymbol{1}_L\\) is a unit vector of length \\(L\\). It is constructed as follows. First assume that the weights are increments of a cdf \\(G\\) on the support \\([0,1]\\); that is,\n\\[\\begin{equation}\nw_l = G(l/L) - G((l - 1)/L), \\quad l = 1,..., L.   \n\\end{equation}\\]\nNext place a Dirichlet process prior on \\(G\\), denoted as \\(DP(\\boldsymbol{\\alpha}_0, \\boldsymbol{G}_0)\\), where \\(\\alpha_0 &gt; 0\\) and \\(\\boldsymbol{G}_0 = Beta(a_0, b_0)\\). Then the vector of weights follows a Dirichlet distribution with parameter vector \\(\\alpha_0 {(a_1, ..., a_L)}^T\\), where \\(\\alpha_0 &gt; 0\\) and \\(a_l = G_0(l/L) - G_0((l - 1)/L)\\), for \\(l = 1,..., L\\). Together, these parameters determine how the data is allocated across the \\(l\\) intervals on the support of the weight distribution and the initial shape of the distribution. Using the Dirichlet process as a nonparametric prior for \\(G\\) allows for general distributional shapes and thus provides flexibility in estimating the mixture weights.\n\n\n3.2.2 Transition Kernels\nEach transition kernel in \\(\\ref{eq:cond_distribution}\\) corresponds to the distribution for a random pair \\((U_l, V_l)\\), for \\(l = 1,..., L\\). That is, \\(f_l \\equiv f_{U_l | V_l}\\), where \\(f_l\\) denotes the transition kernel and \\(f_{U_l | V_l}\\) is the associated conditional density. Necessary and sufficient conditions for constant first and second moments are difficult to establish. Zheng, Kottas, and Sansó (2022) offer an alternative condition on the marginal densities of bivariate distributions that define the transition kernels, simplifying implementation while avoiding restrictive constraints on the parameter space. Proposition \\(\\ref{prop:proposition1}\\) states that if a stationary marginal density \\(f_X\\) that corresponds to the marginal densities of a bivariate random vector \\((U_l, V_l)\\), for all \\(l\\), can be identified, then the resulting time series is first-order strictly stationary (Zheng, Kottas, and Sansó 2022).\nIn addition, Proposition \\(\\ref{prop:proposition1}\\) applies to continuous, discrete, or mixed distributions.\nBuilding on Proposition \\(\\ref{prop:proposition1}\\), Zheng, Kottas, and Sansó (2022) outline two general methods for constructing transition kernels: the bivariate distribution method and the conditional distribution method. The bivariate distribution method identifies a bivariate distribution of \\((U_l, V_l)\\) such that the marginal densities \\(f_{U_l}\\) and \\(f_{V_l}\\) are equal to a pre-specified stationary marginal density \\(f_X\\) for \\(l\\)th transition kernel. The conditional distribution method finds compatible conditional densities, \\(f_{U_l|V_l}\\) and \\(f_{V_l|U_l}\\), to specify the bivariate density of \\((U_l, V_l)\\) for the \\(l\\)th transition kernel.\nThe Gaussian MTD model, for example, is constructed via the bivariate distribution method. Under marginal \\(f_x(x) = N(x | \\mu, \\sigma^2)\\), the Gaussian MTD model can be constructed as\n\\[\\begin{equation}\nf(x_t | \\textbf{x}^{t-1}) = \\sum_{l=1}^L w_l N(x_t | (1 - \\rho_l) \\mu + \\rho_l x_{t-l}, \\sigma^2 (1 - \\rho_l^2)).\n\\end{equation}\\]",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter1/background.html#bayesianimplementation",
    "href": "chapter1/background.html#bayesianimplementation",
    "title": "3  Background",
    "section": "3.3 Bayesian Implementation",
    "text": "3.3 Bayesian Implementation\n\n3.3.1 Hierarchical Model Formulation\nInference is facilitated through a set of latent variables, which identify kernels within the structured mixture model. Let \\({\\{Z_t\\}}_{t=L+1}^n\\) be the set of latent variables, where each \\(Z_t\\) has a discrete distribution with support \\(\\{1,...,L\\}\\). \\(Z_t = l\\) selects the \\(l\\)th kernel through a random mechanism that is not directly observable. In addition, \\(p(z_t | \\boldsymbol{w}) = \\sum_{l=1}^L w_l \\delta_l(z_t)\\), where \\(w = {(w_1,..., w_L)}^T\\), and \\(\\delta_l(z_t) = 1\\) if \\(z_t = l\\) and \\(0\\) otherwise. Based on the specific value of \\(z_t\\), \\(\\delta_l(z_t)\\) selects the corresponding \\(w_l\\).\nThe full Bayesian model is completed by the specification of prior distributions for the parameters \\(\\boldsymbol{\\theta}\\). The priors for \\(\\boldsymbol{\\theta}\\) depend on the form of the kernels \\(f_l\\). For the cdf-based weights, the priors for \\(\\boldsymbol{w}\\) is \\(CDP(\\boldsymbol{w} | \\alpha_0, a_0, b_0)\\).\n\n\n3.3.2 Model Estimation and Prediction\nThe posterior distribution of the parameters, based on the conditional likelihood, is\n\\[\\begin{equation}\n\\begin{split}\np(\\boldsymbol{w}, \\boldsymbol{\\theta}, {\\{z_t\\}}_{t=L+1}^n | D_n) \\propto \\pi_w(\\boldsymbol{w}) \\prod_{l=1}^L \\pi_l(\\boldsymbol{\\theta}_l)\n\\prod_{t=L+1}^n \\Biggl\\{ f_{z_t} (x_t | x_{t-z_t}, \\boldsymbol{\\theta}_{z_t}) \\sum_{l=1}^L w_l \\delta_l(z_t) \\Biggl\\}.\n\\end{split}\n\\end{equation}\\]\n\\(D_n = {\\{x_t\\}}_{t=L+1}^n\\) is the data. In this general framework, full simulation-based Bayesian estimation and prediction can be achieved using Markov chain Monte Carlo (MCMC) algorithms.\nConditioning on \\({\\{z_t\\}}_{t=L+1}^n\\) and \\(\\boldsymbol{w}\\), the posterior full condition for each \\(\\boldsymbol{\\theta}_l\\) depends on the specific form of the kernel \\(f_l\\), which will be presented in Chapter 5. Conditioning on \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{w}\\), the posterior full condition of each latent variable \\(Z_t\\) is a discrete distribution on \\(\\{1, ..., L\\}\\) with probabilities proportional to \\(w_l f_l(x_t | x_{t-l}, \\boldsymbol{\\theta})\\). Conditioning on \\({\\{z_t\\}}_{t=L+1}^n\\) and \\(\\boldsymbol{\\theta}\\), the posterior full condition for \\(\\boldsymbol{w}\\) depends only on \\(M_l = |\\{ t: z_t = l \\}|\\), for \\(l = 1,..., L\\), where \\(|\\{ \\cdot \\}|\\) is the carnality or the size of the set \\(\\{ \\cdot \\}\\).\nTurning to predictions for future values, the one-step-ahead posterior predictive density is\n\\[\\begin{equation}\n\\begin{split}\n\\label{eq:posteriorpredictive}\np(x_{n+1} | D_n) = \\int \\int \\Biggl\\{ \\sum_{l=1}^L w_l f_l (x_{n+1} | x_{n + 1 - l}, \\boldsymbol{\\theta}_l) \\Biggl\\}\np(\\boldsymbol{\\theta} , \\boldsymbol{w} | D_n) d \\boldsymbol{\\theta} d \\boldsymbol{w}.\n\\end{split}\n\\end{equation}\\]\nThe \\(k\\)-step-ahead posterior predictive density, which incorporates the uncertainty from the parameter estimation and the predictions of the previous \\((k-1)\\) out-of-sample values, can be obtained by extending the posterior predictive density in \\(\\ref{eq:posteriorpredictive}\\).\nOur work builds upon the MTD time series model by Zheng, Kottas, and Sansó (2022) and draws inspiration from the nearest-neighbor mixture processes (NNMP) and the discrete nearest-neighbor mixture processes (DNNMP) spatial models developed by the same authors (Zheng, Kottas, and Sansó 2023b, 2023a). The MTD model by Zheng, Kottas, and Sansó (2022) includes Gaussian, Poisson, negative binomial MTD, and Lomax MTD regression models, with applications to simulated data, crime incidents and precipitation prediction. Their flexibility offers potential for broader applications. However, to satisfy Proposition \\(\\ref{prop:proposition1}\\), for certain invariant marginal distributions, the transition kernel may either require careful construction or can result in a form that is not explicitly defined or too complex. The NNMP and DNNMP models effectively use copula to model dependence and marginals separately. Building onto the class of MTD models and motivated by the class of NNMP and DNNMP models, we propose to incorporate copula-based transition kernels. The proposed model is presented in the next chapter.\n\n\n\n\nBerchtold, André. 2001. “Estimation in the Mixture Transition Distribution Model.” Journal of Time Series Analysis 22 (4): 379–97.\n\n\nBerchtold, André, and Adrian Raftery. 2002. “The Mixture Transition Distribution Model for High-Order Markov Chains and Non-Gaussian Time Series.” Statistical Science 17 (3): 328–56.\n\n\nD’Amico, Guglielmo, Riccardo De Blasis, and Filippo Petroni. 2023. “The Mixture Transition Distribution Approach to Networks: Evidence from Stock Markets.” Physica A: Statistical Mechanics and Its Applications 632: 129335.\n\n\nJordan, Michael I. 2004. “Graphical Models.” Statistical Science 19 (1): 140–55.\n\n\nLe, Nhu D, R Douglas Martin, and Adrian Raftery. 1996. “Modeling Flat Stretches, Bursts Outliers in Time Series Using Mixture Transition Distribution Models.” Journal of the American Statistical Association 91 (436): 1504–15.\n\n\nRaftery, Adrian. 1985a. “A Model for High-Order Markov Chains.” Journal of the Royal Statistical Society Series B: Statistical Methodology 47 (3): 528–39.\n\n\n———. 1985b. “A New Model for Discrete-Valued Time Series: Autocorrelations and Extensions.” Rassegna Di Metodi Statistici Ed Applicazioni 3 (4): 149–62.\n\n\n———. 1994. “Change Point and Change Curve Modeling in Stochastic Processes and Spatial Statistics.” Journal of Applied Statistical Science 1 (4): 403–23.\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On Construction and Estimation of Stationary Mixture Transition Distribution Models.” Journal of Computational and Graphical Statistics 31 (1): 283–93.\n\n\n———. 2023a. “Bayesian Geostatistical Modeling for Discrete-Valued Processes.” Environmetrics 34 (7): e2805.\n\n\n———. 2023b. “Nearest-Neighbor Mixture Models for Non-Gaussian Spatial Processes.” Bayesian Analysis 18 (4): 1191–1222.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter1/proposed.html",
    "href": "chapter1/proposed.html",
    "title": "4  Proposed Method: Copula-Based Gamma MTD Models",
    "section": "",
    "text": "4.1 Copula\nThe proposed MTD model extends the original MTD model by incorporating copulas into the transition kernels. By allowing dependence structures to be modeled separately from the marginal distribution, the proposed approach enables a choice of copula families that effectively capture the data’s dependence structures while allowing flexibility in marginal selection, thereby enhancing modeling capabilities and flexibility.\nThe invariant condition in Proposition \\(\\ref{prop:proposition1}\\) is achieved using the bivariate distribution approach, which specifies the stationary density \\(f_X\\) as the marginal densities of \\((U_l, V_l)\\), \\(f_{U_l}\\) and \\(f_{V_l}\\), for all \\(l\\). This is facilitated by the use of a copula, which separates the marginal behavior of the random variables from their dependence structure.\nCopula is a multivariate cumulative distribution function where its marginal distribution of each random variable follows a uniform distribution on the interval \\([0, 1]\\). Copulas are useful for modeling dependence between random variables. Copulas decompose any joint distribution \\(F\\) into two parts: the copula \\(C\\) and the marginal distributions \\(F_j\\). The theoretical groundwork for copulas is rooted in Sklar’s theorem Sklar (1959).\nIn the bivariate setting, as outlined in the bivariate distribution approach, the joint density of \\((U_l, V_l)\\) is \\(f_{U_l, V_l} (x_t, x_{t-l})\\) \\(= c(x_t, x_{t-l})\\) \\(f_{U_l}(x_t) f_{V_l}(x_{t-l})\\). By applying the law of conditional probability, the conditional density of \\(U_l\\) given \\(V_l\\) is then \\(f_{U_l | V_l}(x_t | x_{t-l}) = c(x_t, x_{t-l}) f_{U_l}(x_t) f_{V_l}(x_{t-l}) / f_{V_l}(x_{t-l})\\) \\(= c(x_t, x_{t-l}) f_{U_l}(x_t)\\). Given a pre-specified stationary marginal density \\(f_X\\), and replace \\(f_{U_l}\\) and \\(f_{V_l}\\) with \\(f_X\\), for every \\(x_t\\) and for all \\(l\\). For \\(t &gt; L\\), the proposed copula-based MTD model specifies the conditional distribution as\n\\(c_l (x_t, x_{t-l})\\) is the copula density evaluated at \\(x_t\\) and \\(x_{t-l}\\), and \\(f_X(x_t)\\) is the stationary marginal density evaluated at \\(x_t\\). Compared to \\(\\ref{eq:cond_distribution}\\), the transition kernel, \\(f_l\\), is now replaced by two components: the copula density \\(c_l\\), and the stationary marginal density \\(f_X\\). The copula captures the dependence between current and lagged value and controls the strength of the dependence through a dependence parameter. Stationary marginal density describes the marginal behavior of the current value. A variety of copula families and marginal distributions are available for choice. Without much modifications to our proposed model, the marginal distribution can take any form, but is limited to continuous distributions.\nWe consider the Gaussian copula with the dependence parameter \\(\\rho\\). A Gaussian copula for \\((X_1, X_2)\\) is \\[\\begin{equation}\n    C(u_1, u_2 |  \\rho) = \\Phi_2 (\\Phi^{-1} (u_1),  \\Phi^{-1} (u_2) | \\rho)\n\\end{equation}\\]\n\\(u_j = F_j(x_j)\\) are standard uniform distribution, where \\(F_j\\) is the marginal cdf of \\(X_j\\), for \\(j = 1,2\\). \\(\\Phi_2\\) is the cdf of a bivariate standard Gaussian distribution with the dependence parameter \\(\\rho \\in (-1, 1)\\), and \\(\\Phi\\) is the cdf of a univariate standard Gaussian distribution. If both \\(X_1\\) and \\(X_2\\) are continuous variables, the copula has the density \\[\\begin{equation}\n\\begin{split}\n    c(\\Phi^{-1} (u_1), \\Phi^{-1} (u_2) | \\rho) = \\frac{1} {\\sqrt{(1 - \\rho^2)}} \\exp{\\Biggl( \\frac{2 \\rho \\Phi^{-1} (u_1) \\Phi^{-1} (u_2) - \\rho^2\\{ {(\\Phi^{-1} (u_1))}^2 + {(\\Phi^{-1} (u_2))}^2 \\}} {2 (1 - \\rho^2)} \\Biggr)}.\n\\end{split}\n\\end{equation}\\]",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Proposed Method: Copula-Based Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "chapter1/proposed.html#marginal-distribution",
    "href": "chapter1/proposed.html#marginal-distribution",
    "title": "4  Proposed Method: Copula-Based Gamma MTD Models",
    "section": "4.2 Marginal Distribution",
    "text": "4.2 Marginal Distribution\nWe choose Gamma with the shape, \\(\\alpha\\), and the rate parameter, \\(\\beta\\), as the marginal distribution, i.e., \\(Gamma (\\alpha, \\beta)\\) with mean \\(\\alpha / \\beta\\) and variance \\(\\alpha / \\beta^2\\). Figure 4.1 illustrates gamma distributions with varying shape and rate parameters, which are subsequently used in the simulation studies. The Gamma distribution is useful for modeling positively-skewed, non-negative data, such as failure times, runoff amounts, and insurance claims. Figure 1.1 illustrates a positively-skewed time series dataset of wind speeds, motivating the model design.\n\n\n\n\n\n\nFigure 4.1: Probability density function (PDF) of the gamma distribution with varying shape and rate parameters\n\n\n\nWhile we use a Gaussian copula with a Gamma marginal distribution to illustrate the structure of the proposed model, other choices of copulas and marginal distributions can also be employed without requiring significant modifications to the proposed model. For example, one could develop a Gumble copula with a Gamma MTD model or a Gaussian copula with a Beta MTD model, among others. As copula modeling constitutes a substantial research area beyond the scope of this work, we refer the reader to Joe (2014) for more details.\n[Write this in Discussion too.]\n\n\n\n\nJoe, Harry. 2014. Dependence Modeling with Copulas. CRC press.\n\n\nSklar, M. 1959. “Fonctions de répartition à n Dimensions Et Leurs Marges.” In Annales de l’ISUP, 8:229–31. 3.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Proposed Method: Copula-Based Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "chapter1/computation.html",
    "href": "chapter1/computation.html",
    "title": "5  Overview of MCMC Algorithms",
    "section": "",
    "text": "The full Bayesian model is completed by the specification of prior distributions for the parameters \\(\\alpha\\), \\(\\beta\\), \\(\\boldsymbol{\\rho}\\), and \\(\\boldsymbol{w}\\), where \\(\\alpha\\) and \\(\\beta\\) are parameters of the gamma marginals, and \\(\\boldsymbol{\\rho}\\) and \\(\\boldsymbol{w}\\) are the dependence and weight parameters, respectively. For the copula-based Gamma MTD model, the priors are specified as \\(Gamma(\\alpha | u_{\\alpha}, v_{\\alpha})\\), \\(Gamma(\\beta | u_{\\beta}, v_{\\beta})\\), and \\(Unif(\\rho_l |-1, 1)\\). For the cdf-based weights, the prior is \\(CDP(\\boldsymbol{w} | \\alpha_0, a_0, b_0)\\).\nThe parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\boldsymbol{\\rho}\\) are updated using a slice sampler (Neal 2003). Following the definition in Equation \\(\\ref{eq:cond_distribution_copula}\\), denote \\(f_l (x_t | x_{t-l})\\) as \\(f_l (x_t | x_{t-l}) = c_l (x_t, x_{t-l}) f_X(x_t)\\), where \\(f_l\\) is the transition kernel, \\(c_l\\) is the copula density, and \\(f_X\\) is the stationary marginal density. The posterior full conditional distributions for the marginal parameters \\(\\alpha\\) and \\(\\beta\\) are proportional to \\(Gamma(\\alpha | u_{\\alpha}, v_{\\alpha}) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\) and \\(Gamma(\\beta | u_{\\beta}, v_{\\beta})\\) \\(\\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\), respectively. The posterior full conditional distribution for each of the dependence parameters \\(\\boldsymbol{\\rho}\\) is proportional to \\(Unif(\\rho_l |-1, 1) \\prod_{t:z_t = l} c_l (x_t, x_{t-l})\\).\nFor the latent variables \\({\\{z_t\\}}_{t=L+1}^n\\), the posterior full conditional for each \\(z_t\\) is a discrete distribution on \\(\\{1, ..., L\\}\\), where the probability of \\(z_t = l\\), denoted by \\(q_l\\), is proportional to \\(w_l c_l (x_t, x_{t-l})\\), for \\(l = 1,..., L\\). The posterior full conditional distribution for weight parameters \\(\\boldsymbol{w}\\), under the cdf-based prior, is \\(Dirichlet (\\boldsymbol{\\alpha})\\), where \\(\\boldsymbol{\\alpha} = (\\alpha_0 a_1 + M_1, ..., \\alpha_0 a_L + M_L)\\).\nThe MCMC algorithm, adapted from Zheng’s source code for the MTD model (Zheng, Kottas, and Sansó 2022), is written in \\(\\texttt{R}\\), with certain functions written in \\(\\texttt{C++}\\). Algorithm \\(\\ref{alg:mcmc}\\) requires data, mtd order, hyperparameters of the priors for \\(\\alpha\\), \\(\\beta\\), \\(\\boldsymbol{w}\\), and starting values for \\(\\alpha\\), \\(\\beta\\), \\(\\boldsymbol{\\rho}\\). It also requires tuning parameters for the slice sampler, including step size and upper bounds for \\(\\alpha\\) and \\(\\beta\\), along with the general MCMC settings such as number of iterations, burn-in period, and thinning interval. The algorithm outputs posterior samples of \\(\\alpha\\), \\(\\beta\\), \\(\\boldsymbol{\\rho}\\) and \\(\\boldsymbol{w}\\). Asterisk (*) denotes modification of the algorithm.\n\n\n\n\nNeal, Radford M. 2003. “Slice Sampling.” The Annals of Statistics 31 (3): 705–67.\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On Construction and Estimation of Stationary Mixture Transition Distribution Models.” Journal of Computational and Graphical Statistics 31 (1): 283–93.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Overview of MCMC Algorithms</span>"
    ]
  },
  {
    "objectID": "chapter1/simulation.html",
    "href": "chapter1/simulation.html",
    "title": "6  Simulation Studies",
    "section": "",
    "text": "6.1 Simulation Settings\nThe goal of simulation studies is to assess accuracy and performance of the proposed model in Chapter 4. We examine a range of settings by varying the parameters for weight, dependence, and marginal distribution.\nWith weight parameters \\(\\boldsymbol{w}\\), dependence parameters for Gaussian copula \\(\\boldsymbol{\\rho}\\), shape \\(\\alpha\\) and rate parameter \\(\\beta\\), we generate \\(n = 2000\\) observations from the copula-based Gamma MTD model. For model fitting, we set the order \\(L = 5\\) and consider the Gaussian copula with gamma marginals.\nTo be consistent with the original MTD studies, we run the Gibbs sampler for \\(165,000\\) iterations, discard the first \\(5000\\) iterations as burn-in, and collect samples every \\(20\\) iterations, resulting in \\(8000\\) iterations per MCMC chain. To ensure that we can assess MCMC convergence and obtain more precise estimates of parameters, we also run four MCMC chains with \\(8000\\) iterations each for all of the following scenarios in Tables (?tbl-scenarios-description, ?tbl-scenarios), which contain the description and the summary of scenarios, respectively.\nIn all scenarios, we use the cdf-based Dirichlet process (CDP) prior on the weights. Other prior choices, such as the Dirichlet prior and the truncated stick-breaking (SB) prior are readily available, but the original MTD studies has shown that SB and CDP priors give more precise estimates.\nAll scenarios were initially analyzed using a single replicate. Scenarios 1 and 2 were further evaluated with multiple replicates to assess coverage and robustness. Each replicate consisted of a new synthetic dataset generated with the same underlying parameters but different random seeds. Specifically, we ran the models on 10 independently generated replicates for Scenarios 1 and 2 to evaluate the consistency and robustness of the results, ensuring comparability across scenarios.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "chapter1/simulation.html#sec-ch1-simu-res",
    "href": "chapter1/simulation.html#sec-ch1-simu-res",
    "title": "6  Simulation Studies",
    "section": "6.2 Simulation Results",
    "text": "6.2 Simulation Results\n\n6.2.1 Convergence Diagnostics\nScenario 1 in ?tbl-scenarios serves as an example to show and track convergence and has the same setup as Scenario 1 in the original MTD studies.\nTables (?tbl-w-table-s1, ?tbl-rho-table-s1, ?tbl-mar-table-s1) present the posterior estimates and convergence diagnostics for the parameters related to weight, dependence, and marginal distribution, respectively. We defer the discussion of the estimates of the posterior mean and standard deviation (mean and SD) until a later section. There is no evidence of lack of convergence for all parameters (Gelman-Rubin statistic \\(R\\) and its upper CI \\(\\leq 1.1\\)). The simulation error of the estimates is also negligible for all parameters (Naive SE and Time-series SE are close to zero).\nGelman–Rubin convergence diagnostic and ACF plots (Figure B.1, Figure B.2, Figure B.3) can be found in the Section B.1.1. In Scenario 1, the chains converge more rapidly for the parameters related to the marginal distribution, achieving convergence at around \\(2000\\) iterations. The chains converge more slowly for the parameters related to weight and dependence, especially at later lags. Nevertheless, all weight and dependence parameters reach convergence by \\(8000\\) iterations. Similar patterns emerge across all other scenarios. Trace and density plots (Figure B.4, Figure B.5, Figure B.6) are also included in Section B.1.1.\n\n\n6.2.2 Weight and Dependence Parameters for Copula\nScenarios 1, 2, 1.3, 1.4, 1.5, 1.6 in ?tbl-scenarios are employed to demonstrate the effectiveness of weight and dependence construction, as well as their interplay.\nScenario 1 and 2 share the same setup as the original MTD studies, where weight and dependence are compatible. In Scenario 1, we consider exponentially decreasing weights. In Scenario 2, we consider an uneven arrangement of the relevant lags.\nWe explore additional scenarios to investigate outcomes when weight and dependence are incompatible, as well as cases where they are compatible but follow rarely observed patterns. In Scenario 1.3, we assign equal weights while preserving a decreasing dependence structure. In Scenario 1.4, we retain exponentially decreasing weights but reverse the direction of dependence. In Scenario 1.5, both weights and dependence are set to increase. In Scenario 1.6, we set both weights and dependence to be equal.\nWe first examine two cases where the weight parameters and the dependence parameters are compatible. As shown in (a), (b) of Figure 6.1, the results appear reasonable; that is, the estimates are consistent with the true values, with minor discrepancies. Nevertheless, the differences are minimal, and the \\(95\\%\\) posterior credible intervals cover the true value for both weight and dependence across all lags.\nWe then explore additional scenarios to investigate the outcomes when they are not compatible. As shown in (c) and (d) of Figure 6.1, the results appear unusual, with some noticeable discrepancies at later lags. The \\(95\\%\\) posterior credible intervals cover the true value for both weight and dependence for most lags.\nMoving on to the remaining cases where weights and dependence are compatible, though they follow patterns that are rarely observed in real-world settings. As shown in (e) and (f) of Figure 6.1, the results appear reasonable, with minimal discrepancies. The \\(95\\%\\) posterior credible intervals cover the true value for both weight and dependence for most lags.\nIn these scenarios, greater weight on a lag yields narrower \\(95\\%\\) posterior credible interval (CI) for that lag, while lesser weight results in wider CI. If more weight is placed on a lag, it contributes more to the current value. This increased contribution provides the model with more information to estimate its influence, resulting in narrower CI. Conversely, with less information available to estimate its influence, the CI widens and approaches the prior distribution.\n\n\n\n\n\n\nFigure 6.1: (a), (b) Results for Scenarios 1 and 1.2: default setup; (c), (d) Scenarios 1.3 and 1.4: incompatible weight and dependence; (e), (f) Scenarios 1.5 and 1.6: compatible, but rarely observed patterns. (Left) Dashed lines are true weights, dot-dashed lines are prior means, solid lines are posterior means, and polygons are \\(95\\%\\) posterior credible intervals. (Right) Dashed (black) lines are true dependence, dot-dashed (red) lines are prior means, solid (blue) lines are posterior means, and (purple) polygons are \\(95\\%\\) posterior credible intervals.\n\n\n\n\n\n6.2.3 Parameters for Marginal Distributions\nScenario 3 to 6 in ?tbl-scenarios are used to evaluate the shape and the rate parameter for the gamma marginal distribution. Scenario 7 through 9 are used to evaluate these parameters in cases with high skewness.\nIn Scenario 3 to 9, we revert to the same settings for weight and dependence as used in Scenario 1. That is, we fix \\(w_i \\propto \\exp(-i), i = 1,..., 5\\) and \\(\\rho_l = (0.7, 0.5, 0.3, 0.1, 0.1)\\).\nThe slice sampler may encounter difficulties when the target distribution is not evaluable. In particular, skewness of the target distribution may reduce the efficiency of the slice sampler by inducing correlations between successive samples (Planas and Rossi 2024). We explore additional scenarios to identify where the algorithm may fail. Scenarios 7, 8, and 9, which illustrate increasing skewness, are highlighted as part of Figure 4.1 and Figure B.8.\nAs shown in ?tbl-mar-table-s3456, the results appear reasonable; that is, convergence has been achieved and the estimates are consistent with the true values. As shown in ?tbl-mar-table-s789, the results also appear reasonable. Additional plots (Figure B.7, Figure B.8) can be found in the Section B.1.3.\n\n\n6.2.4 Sensitivity Analysis\nFor the prior sensitivity analysis, we re-run Scenario 1 using five different sets of priors. ?tbl-prior-analysis presents the setups and descriptions of these prior specifications. Figure 6.2 illustrates some examlples.\nResults appear reasonable, and the estimates are consistent with the true values, indicating that the model is robust to the choice of prior.\n\n\n\n\n\n\nFigure 6.2: Examples of Prior Distributions for Scenario 1: (a) \\(\\alpha\\), centered at a mean of \\(7\\), and (b) \\(\\beta\\), with a mean of \\(1\\).\n\n\n\n\n\n6.2.5 Coverage Assessment\nTo compute coverage rates, for each of the \\(10\\) replicates, we first combine the \\(4\\) chains of \\(8000\\) posterior samples per parameter, then calculate the \\(95\\%\\) credible interval from the combined samples, and record whether the true parameter value falls within this interval. The overall coverage is the proportion of replicates in which the true value is contained within the interval.\nAs shown in Tables (?tbl-coverage-1 and ?tbl-coverage-2), the \\(95\\%\\) credible intervals for all parameters successfully contain the true values in most replicates across both scenarios. Most parameters achieve full coverage, with a few slightly below \\(1\\), indicating that the posterior intervals reliably capture the true parameter values.\n\n\n\n\nPlanas, Christophe, and Alessandro Rossi. 2024. “The Slice Sampler and Centrally Symmetric Distributions.” Monte Carlo Methods and Applications 30 (3): 299–313.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "chapter1/prediction.html",
    "href": "chapter1/prediction.html",
    "title": "7  Prediction",
    "section": "",
    "text": "?tbl-pred-gamma summaries the \\(95\\%\\) one-step ahead posterior predictive intervals for Scenario 1 through 9. As presented in this table, the model appropriately captures the predictive uncertainty across all scenarios. Figure 7.1 illustrate these intervals for Scenario 1 and 2. The differences in the observed patterns arise from the specific configurations of the weight and dependence parameters in each scenario; Scenario 1 employs exponentially decreasing weight and dependence, while Scenario 2 adopts an uneven arrangement of weight and dependence across lags. Additional plots illustrating the intervals for Scenarios 3 through 9 (Figure D.1, Figure D.2) are provided in the Appendix D.\n\n\n\n\n\n\nFigure 7.1: \\(95\\%\\) one-step ahead posterior predictive intervals for Gamma Scenario 1 and 2.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prediction</span>"
    ]
  },
  {
    "objectID": "chapter1/discussion.html",
    "href": "chapter1/discussion.html",
    "title": "8  Discussion",
    "section": "",
    "text": "In this work, we review a broad class of stationary MTD models and propose a novel copula-based MTD model that builds upon the existing framework. We also present the algorithms and simulation studies, which demonstrate promising results across various scenarios. The advantage of our proposed approach is that, by incorporating copulas into the existing MTD models, the dependence structure and the marginal distribution can be modeled separately, allowing the marginal distribution to be any continuous form.\nIn real-world settings, some data exhibit zero inflation and requires modeling with a mixture model with a point mass at zero. For example, medical costs, insurance claims, precipitation amounts, as well as transportation safety measures such as lane departure severity scores (Mills 2013) and vehicle deceleration during braking (Feng 2020). Failure to address these issues undermines model robustness and results. Furthermore, the copula approach may encounter issues when handling a large number of zeros. In such case, the marginal distribution needs to be re-constructed. This motivates the methodological developments introduced in Part II, where we develope a model that addresses the issues of zero inflation.\nMoreover, a previous study (Hassan 2021) found comparable predictive performance for disease spread between the probabilistic MTD model and the deep learning long short-term memory (LSTM) network. We aim to compare our approach to this alternative method. This comparative analysis is presented in Part III, where we compare the performance of our proposed MTD models against the LSTMs through simulation studies and real-world data applications.\n\n\n\n\nFeng, Tianshu. 2020. Zero-Inflated Models for Semi-Continuous Transportation Data. University of Washington.\n\n\nHassan, Mohamed Yusuf. 2021. “The Deep Learning LSTM and MTD Models Best Predict Acute Respiratory Infection Among Under-Five-Year Old Children in Somaliland.” Symmetry 13 (7): 1156.\n\n\nMills, Elizabeth Dastrup. 2013. Adjusting for Covariates in Zero-Inflated Gamma and Zero-Inflated Log-Normal Models for Semicontinuous Data. The University of Iowa.",
    "crumbs": [
      "Models for Forecasting Skewed Time Series",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapter2/intro.html",
    "href": "chapter2/intro.html",
    "title": "9  Introduction",
    "section": "",
    "text": "Zero-inflated data are characterized by an excess of zero values. In these data types, observations often feature a point mass at zero, followed by values from a separate distribution that can be discrete, continuous, or otherwise non-zero. Zero-inflated count data frequently occur in a wide range of domains, including finance, economics, healthcare, transportation, and ecology. While zero-inflated count data have been studied extensively, zero-inflated continuous, or semicontinuous, data also arise frequently in practice. Examples of semicontinuous data are medical expenditures (Duan et al. 1983; Neelon, Zhu, and Neelon 2015; Neelon, O’Malley, and Smith 2016a, 2016b; Liu et al. 2019), insurance claims (Shi and Yang 2018; Yang 2022), precipitation amounts (Hyndman and Grunwald 2000; Abraham and Tan 2009; Dzupire, Ngare, and Odongo 2018; Kaewprasert, Niwitpong, and Niwitpong 2024), lane departure severity scores (Mills 2013), and vehicle deceleration during braking (Feng 2020).\nThere are two classes of models designed to handle data with excessive zeros: two-component, zero-inflated (ZI) models (Lambert 1992) and two-part, hurdle models (Mullahy 1986). Both models employ a mixture of a binary component modeling zeros and a separate component modeling non-zero values, which can be either count or continuous. The key distinction lies in the source of zeros. In ZI models, zeros may arise from both the binary and non-zero components, whereas in hurdle models, zeros occur exclusively in the binary component, with the non-zero component restricted to non-zero values.\nBuilding upon the architecture of the MTD model introduced in 2022 by Zheng, Kottas, and Sansó (2022), in Part I, we propose the copula-based Gamma MTD model, which enables flexible dependence modeling and accommodates arbitrary continuous marginals, thereby enhancing modeling capabilities and flexibility. However, while this framework addresses the challenge of constructing a flexible transition kernel for non-Gaussian marginal distributions, it remains limited in handling excessive zeros commonly observed in real-world continuous data.\nTo address this limitation, we propose reconstructing the marginal distribution to account for zero-inflation. Our approach is similar to hurdle models in that it models zero and non-zero values separately. Unlike hurdle models, however, it applies a soft threshold that replaces zeros with small non-zero values rather than generating exact zeros. Building on the continuous extension (CE) approach (Denuit and Lambert 2005), this technique transforms zero-inflated marginal distributions into continuous distributions, thereby mitigating the identifiability issues that copulas face when modeling discrete or mixed marginals (Genest and Nešlehová 2007). This CE-based reformulation enables copulas to effectively capture complex dependencies while accurately modeling zero-inflated continuous marginals. The proposed copula-based zero-inflated MTD model extends the copula-based MTD model by accommodating for zero-inflation, thereby enhancing its applicability and flexibility for handling mixed data with excess zeros.\nThe rest of the chapter is organized as follows. We review several zero-inflated (ZI) count and continuous models for dependent data, as well as the continuous extension (CE) approach in Chapter 10. We present the proposed model in Chapter 11 and provide an overview of the MCMC algorithm for parameter estimation in Chapter 12. We present the results of various simulations conducted to assess the accuracy and performance of the proposed model in Chapter 13 and discuss the model’s predictive capabilities, including uncertainty quantification, in Chapter 14. Finally, we conclude with a discussion in Chapter 15. Appendix H provides the instruction for installing the extended mtd package.\n\n\n\n\nAbraham, Zubin, and Pang-Ning Tan. 2009. “A Semi-Supervised Framework for Simultaneous Classification and Regression of Zero-Inflated Time Series Data with Application to Precipitation Prediction.” In 2009 IEEE International Conference on Data Mining Workshops, 644–49. IEEE.\n\n\nDenuit, Michel, and Philippe Lambert. 2005. “Constraints on Concordance Measures in Bivariate Discrete Data.” Journal of Multivariate Analysis 93 (1): 40–57.\n\n\nDuan, Naihua, Willard G Manning, Carl N Morris, and Joseph P Newhouse. 1983. “A Comparison of Alternative Models for the Demand for Medical Care.” Journal of Business & Economic Statistics 1 (2): 115–26.\n\n\nDzupire, Nelson Christopher, Philip Ngare, and Leo Odongo. 2018. “A Poisson-Gamma Model for Zero Inflated Rainfall Data.” Journal of Probability and Statistics 2018 (1): 1012647.\n\n\nFeng, Tianshu. 2020. Zero-Inflated Models for Semi-Continuous Transportation Data. University of Washington.\n\n\nGenest, Christian, and Johanna Nešlehová. 2007. “A Primer on Copulas for Count Data.” ASTIN Bulletin: The Journal of the IAA 37 (2): 475–515.\n\n\nHyndman, Rob J, and Gary K Grunwald. 2000. “Applications: Generalized Additive Modelling of Mixed Distribution Markov Models with Application to Melbourne’s Rainfall.” Australian & New Zealand Journal of Statistics 42 (2): 145–58.\n\n\nKaewprasert, Theerapong, Sa-Aat Niwitpong, and Suparat Niwitpong. 2024. “Bayesian Confidence Intervals for the Ratio of the Means of Zero-Inflated Gamma Distributions with Application to Rainfall Data.” Communications in Statistics-Simulation and Computation 53 (12): 5780–96.\n\n\nLambert, Diane. 1992. “Zero-Inflated Poisson Regression, with an Application to Defects in Manufacturing.” Technometrics 34 (1): 1–14.\n\n\nLiu, Lei, Ya-Chen Tina Shih, Robert L Strawderman, Daowen Zhang, Bankole A Johnson, and Haitao Chai. 2019. “Statistical Analysis of Zero-Inflated Nonnegative Continuous Data.” Statistical Science 34 (2): 253–79.\n\n\nMills, Elizabeth Dastrup. 2013. Adjusting for Covariates in Zero-Inflated Gamma and Zero-Inflated Log-Normal Models for Semicontinuous Data. The University of Iowa.\n\n\nMullahy, John. 1986. “Specification and Testing of Some Modified Count Data Models.” Journal of Econometrics 33 (3): 341–65.\n\n\nNeelon, Brian, A James O’Malley, and Valerie A Smith. 2016a. “Modeling Zero-Modified Count and Semicontinuous Data in Health Services Research Part 1: Background and Overview.” Statistics in Medicine 35 (27): 5070–93.\n\n\n———. 2016b. “Modeling Zero-Modified Count and Semicontinuous Data in Health Services Research Part 2: Case Studies.” Statistics in Medicine 35 (27): 5094–5112.\n\n\nNeelon, Brian, Li Zhu, and Sara E Benjamin Neelon. 2015. “Bayesian Two-Part Spatial Models for Semicontinuous Data with Application to Emergency Department Expenditures.” Biostatistics 16 (3): 465–79.\n\n\nShi, Peng, and Lu Yang. 2018. “Pair Copula Constructions for Insurance Experience Rating.” Journal of the American Statistical Association 113 (521): 122–33.\n\n\nYang, Lu. 2022. “Nonparametric Copula Estimation for Mixed Insurance Claim Data.” Journal of Business & Economic Statistics 40 (2): 537–46.\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On Construction and Estimation of Stationary Mixture Transition Distribution Models.” Journal of Computational and Graphical Statistics 31 (1): 283–93.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter2/background.html",
    "href": "chapter2/background.html",
    "title": "10  Background",
    "section": "",
    "text": "10.1 ZI Count Models\nZero-inflated count data are prevalent across diverse domains such as finance, economics, healthcare, transportation, and ecology. Examples include claim frequencies in automobile insurance (Chowdhury et al. 2019; Zhang, Pitt, and Wu 2022; Bermúdez and Karlis 2022; Simmachan and Boonkrong 2024; Slime et al. 2025), the number of business service firms within an airport economic zone (Jiang et al. 2018), the frequency of medical service use (Pizer and Prentice 2011; Chatterjee et al. 2018), crash counts or accident frequencies (Dong et al. 2014; Hao, Ya-dong, and Yong 2016; Liu et al. 2018; Mathew and Benekohal 2021), and species abundances (Martin et al. 2005).\nAccordingly, there is a rich literature on zero-inflated count models in these domains. For a comprehensive review of zero-inflated count regression models, as well as zero-inflated count time series, spatial, and multivariate models, we refer the reader to Young, Roemmele, and Yeh (2022) and Young, Roemmele, and Shi (2022), respectively.\nOur primary goal is to develop models for zero-inflated continuous time series data. To provide a foundation for this discussion, we review two zero-inflated count time series models in the next section.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter2/background.html#sec-ch2-background-zi-count-dependent",
    "href": "chapter2/background.html#sec-ch2-background-zi-count-dependent",
    "title": "10  Background",
    "section": "10.2 ZI Count Models for Dependent Data",
    "text": "10.2 ZI Count Models for Dependent Data\n\n10.2.1 State Space Models\nYang, Cavanaugh, and Zamba (2015) propose a state space or dynamic model for zero-inflated count time series. Feng (2020) extends this framework to continuous-valued zero-inflated time series, resulting in the development of the dynamic semi-continuous zero-inflated (DSCZI) model.\nBoth the dynamic model and the MTD model include a latent state and can be classified as state space models. However, they differ in how they capture time dependence. In particular, the dynamic model introduces a continuous latent process that evolves over time according to a autoregressive (AR) process of order \\(p\\), which can be equivalently represented as a \\(p\\)-dimentional AR(\\(1\\)) process. In contrast, the MTD framework represents time dependence via a set of discrete latent variables, each selecting a lag-specific kernel among \\(L\\) kernels. These latent variables determine which lag-specific kernel governs the current state through a random but non-dynamically evolving mechanism. As a result, while the dynamic model represents temporal evolution via a continuous, hidden state process, the MTD model captures time dependence through the dynamics embedded in the lag-specific kernels, with selection governed by a set of discrete, static latent variables.\nIn the standard MTD framework, these latent variables are assumed to be independent, but in some variants, the discrete latent state structure can be dynamic. For example, Bartolucci and Farcomeni (2010) propose a model in which the discrete latent variables follow a hidden Markov chain and note that further generalizations, though possible, often result in models with a large number of parameters and require computationally intensive algorithms for fitting. In addition, with the advancement of modern computing, Yang, Cavanaugh, and Zamba (2015) suggest a full Bayesian framework using a MCMC approach for future work.\n\n\n10.2.2 Copula-Based Markov Models\nAlqawba, Diawara, and Rao Chaganty (2019) and Alqawba and Diawara (2021) propose the copula-based Markov zero-inflated count time series model, utilizing marginal distributions such as the zero-inflated Poisson (ZIP), zero-inflated negative binomial (ZINB), and zero-inflated Conway-Maxwell-Poisson (ZICMP).\nSimilarly, the MTD and NNMP models belong to the class of first-order Markov models. However, copulas were not incorporated into these frameworks until the later work of Zheng, Kottas, and Sansó (2023b) and Zheng, Kottas, and Sansó (2023a). Even then, in the discrete spatial NNMP models, Zheng, Kottas, and Sansó (2023a) utilize copulas in a different manner. More specifically, Alqawba, Diawara, and Rao Chaganty (2019) and Alqawba and Diawara (2021) directly compute the joint PMF of \\((X_t, X_{t-1})\\) and express it using a copula function adapted for discrete variables, from which the conditional probability is obtained by dividing by the marginal of \\(x_{t-1}\\). In contrast, Zheng, Kottas, and Sansó (2023a) adopt the continuous extension approach, associating each discrete variable with a continuous variable. This enables the direct use of copulas in a continuous setting to construct the transition kernel in a structured mixture. Once the continuous variables are introduced, the conditional probability is specified as a mixture over \\(L\\) transition kernels, each constructed from the bivariate random vector \\((U_l, V_l)\\), with the dependence between \\(U_l\\) and \\(V_l\\) captured by a copula function.\nAlthough copula-based Markov models have been widely applied to count and zero-inflated count data in both time series and spatial contexts, their application to zero-inflated continuous data remains largely unexplored.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter2/background.html#zi-continuous-models",
    "href": "chapter2/background.html#zi-continuous-models",
    "title": "10  Background",
    "section": "10.3 ZI Continuous models",
    "text": "10.3 ZI Continuous models\nZero-inflated continuous data frequently appear in domains such as healthcare, insurance, environment, and transportation. While existing literature has primarily focused on zero-inflated count data, there has been relatively less attention given to zero-inflated continuous data. Nevertheless, these studies suggest promising potential for broader applications. For example, Mills (2013) conducts two-part tests for zero-inflated Gamma (ZIG) and zero-inflated log-normal (ZILN) models and applies them to assess driving risk in individuals with neurological conditions. Zhou, Kang, and Song (2020) develope a two-part hidden Markov model and apply it to analyze data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) to enhance prognosis and support early treatment. Feng (2020) applies dynamic semi-continuous zero-inflated (DSCZI) time series model to analyze adaptive cruise control data to investigate drivers’ braking behavior, informing the in-vehicle assistance design. Sun (2020) compares the spatial Gaussian copula model with the kriging and the spatial random forests for zero-inflated forestry inventory prediction, crucial for ecosystem management. More recently, Kaewprasert, Niwitpong, and Niwitpong (2022) and Kaewprasert, Niwitpong, and Niwitpong (2024) construct Bayesian and fiducial intervals and apply them to rainfall data from northern Thailand, informing the design of disaster warning systems. Zou and Young (2024) construct fiducial-based intervals for the ZIG distribution and apply them to lipid profiles in a lung cancer study, enhancing screening and staging accuracy.\nThe DSCZI model by Feng (2020) is built based on the dynamic framework by Yang, Cavanaugh, and Zamba (2015), as discussed in Section 10.2. Conditioning on the latent state, the model uses logistic regression to model the zero values and a Gamma distribution for the positive responses. For parameter estimation, the DSCZI model utilizes the data cloning method (Lele, Dennis, and Lutscher 2007; Lele, Nadeem, and Schmuland 2010; Al-Wahsh and Hussein 2019), which leverages Markov chain Monte Carlo (MCMC) sampling to approximate maximum likelihood estimates, avoiding gradient-based methods due to the intractable marginal likelihood and latent variable complexity.\nAlthough data cloning provides a practical approach for intractable likelihoods and is robust to the choice of prior distributions, it can be computationally expensive and sensitive to the choice of the number of clones (Lele, Dennis, and Lutscher 2007). Moreover, convergence diagnostics can be challenging, and identifiability issues may persist even after cloning (Lele, Dennis, and Lutscher 2007). In contrast, MCMC draws samples from the full posterior distribution, providing a more comprehensive quantification of parameter uncertainty.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter2/background.html#the-continuous-extension-approach",
    "href": "chapter2/background.html#the-continuous-extension-approach",
    "title": "10  Background",
    "section": "10.4 The Continuous Extension Approach",
    "text": "10.4 The Continuous Extension Approach\nThe continuous extension (CE) approach was first proposed by Denuit and Lambert (2005). Under the CE framework, Madsen (2009) introduces a discrete spatial Gaussian copula regresion model and applies it to the Japanese beetle grub data, while Zheng, Kottas, and Sansó (2023a) develop a discrete spatial copula NNMP regression model to study North American Breeding Bird Survey data. Using this approach, each discrete random variable, \\(Y_i\\), is associated with a continuous variable, \\(Y_i^*\\), defined as\n\\[\\begin{equation}\nY_i^* = Y_i - U_i,\n\\end{equation}\\]\nwhere \\(U_i\\) follows a continuous uniform distribution on \\((0, 1)\\), independent of both \\(Y_i\\) and \\(U_j\\), for \\(i \\neq j\\). The resulting \\(Y_i^*\\) is a continuous random variable. Not only does this continuous extension of \\(Y_i\\) preserves all information, but \\(Y^*_i\\) and \\(Y^*_j\\) also retain the dependence structure of \\(Y_i\\) and \\(Y_j\\), as shown by Denuit and Lambert (2005).\nOur work once again builds upon the MTD time series model by Zheng, Kottas, and Sansó (2022) and draws inspiration from the NNMP and the discrete NNMP spatial models developed by the same authors (Zheng, Kottas, and Sansó 2023b, 2023a). Additional motivation comes from the work of Monleon, Madsen, and Wilson (2019) and Sun (2020), which extend the spatial Gaussian copula model of Madsen (2009) by adapting the CE approach for zero-inflated continuous data. Building onto these models, we propose to reconstruct the marginal distribution to accomondate for zero-inflation. We present the proposed model in the next chapter.\n\n\n\n\nAlqawba, Mohammed, and Norou Diawara. 2021. “Copula-Based Markov Zero-Inflated Count Time Series Models with Application.” Journal of Applied Statistics 48 (5): 786–803.\n\n\nAlqawba, Mohammed, Norou Diawara, and N Rao Chaganty. 2019. “Zero-Inflated Count Time Series Models Using Gaussian Copula.” Sequential Analysis 38 (3): 342–57.\n\n\nAl-Wahsh, H, and A Hussein. 2019. “Estimation of Zero-Inflated Parameter-Driven Models via Data Cloning.” Journal of Statistical Computation and Simulation 89 (6): 951–65.\n\n\nBartolucci, Francesco, and Alessio Farcomeni. 2010. “A Note on the Mixture Transition Distribution and Hidden Markov Models.” Journal of Time Series Analysis 31 (2): 132–38.\n\n\nBermúdez, Lluı́s, and Dimitris Karlis. 2022. “Copula-Based Bivariate Finite Mixture Regression Models with an Application for Insurance Claim Count Data.” TEST 31 (4): 1082–99.\n\n\nChatterjee, Saptarshi, Shrabanti Chowdhury, Himel Mallick, Prithish Banerjee, and Broti Garai. 2018. “Group Regularization for Zero-Inflated Negative Binomial Regression Models with an Application to Health Care Demand in Germany.” Statistics in Medicine 37 (20): 3012–26.\n\n\nChowdhury, Shrabanti, Saptarshi Chatterjee, Himel Mallick, Prithish Banerjee, and Broti Garai. 2019. “Group Regularization for Zero-Inflated Poisson Regression Models with an Application to Insurance Ratemaking.” Journal of Applied Statistics 46 (9): 1567–81.\n\n\nDenuit, Michel, and Philippe Lambert. 2005. “Constraints on Concordance Measures in Bivariate Discrete Data.” Journal of Multivariate Analysis 93 (1): 40–57.\n\n\nDong, Chunjiao, Stephen H Richards, David B Clarke, Xuemei Zhou, and Zhuanglin Ma. 2014. “Examining Signalized Intersection Crash Frequency Using Multivariate Zero-Inflated Poisson Regression.” Safety Science 70: 63–69.\n\n\nFeng, Tianshu. 2020. Zero-Inflated Models for Semi-Continuous Transportation Data. University of Washington.\n\n\nHao, Wang, Yang Ya-dong, and Ma Yong. 2016. “Research on the Yangtze River Accident Casualties Using Zero-Inflated Negative Binomial Regression Technique.” In 2016 IEEE International Conference on Intelligent Transportation Engineering (ICITE), 72–75. IEEE.\n\n\nJiang, Yonglei, Adolf KY Ng, Yunpeng Wang, Lu Wang, and Bin Yu. 2018. “Locational Characteristics of Firms in the Business Service Industry in Airport Economic Zones: Case of Shanghai Hongqiao International Airport.” Journal of Urban Planning and Development 144 (1): 04018001.\n\n\nKaewprasert, Theerapong, Sa-Aat Niwitpong, and Suparat Niwitpong. 2022. “Simultaneous Confidence Intervals for the Ratios of the Means of Zero-Inflated Gamma Distributions and Its Application.” Mathematics 10 (24): 4724.\n\n\n———. 2024. “Bayesian Confidence Intervals for the Ratio of the Means of Zero-Inflated Gamma Distributions with Application to Rainfall Data.” Communications in Statistics-Simulation and Computation 53 (12): 5780–96.\n\n\nLele, Subhash R, Brian Dennis, and Frithjof Lutscher. 2007. “Data Cloning: Easy Maximum Likelihood Estimation for Complex Ecological Models Using Bayesian Markov Chain Monte Carlo Methods.” Ecology Letters 10 (7): 551–63.\n\n\nLele, Subhash R, Khurram Nadeem, and Byron Schmuland. 2010. “Estimability and Likelihood Inference for Generalized Linear Mixed Models Using Data Cloning.” Journal of the American Statistical Association 105 (492): 1617–25.\n\n\nLiu, Chenhui, Mo Zhao, Wei Li, and Anuj Sharma. 2018. “Multivariate Random Parameters Zero-Inflated Negative Binomial Regression for Analyzing Urban Midblock Crashes.” Analytic Methods in Accident Research 17: 32–46.\n\n\nMadsen, Lisa. 2009. “Maximum Likelihood Estimation of Regression Parameters with Spatially Dependent Discrete Data.” Journal of Agricultural, Biological, and Environmental Statistics 14: 375–91.\n\n\nMartin, Tara G, Brendan A Wintle, Jonathan R Rhodes, Petra M Kuhnert, Scott A Field, Samantha J Low-Choy, Andrew J Tyre, and Hugh P Possingham. 2005. “Zero Tolerance Ecology: Improving Ecological Inference by Modelling the Source of Zero Observations.” Ecology Letters 8 (11): 1235–46.\n\n\nMathew, Jacob, and Rahim F Benekohal. 2021. “Highway-Rail Grade Crossings Accident Prediction Using Zero Inflated Negative Binomial and Empirical Bayes Method.” Journal of Safety Research 79: 211–36.\n\n\nMills, Elizabeth Dastrup. 2013. Adjusting for Covariates in Zero-Inflated Gamma and Zero-Inflated Log-Normal Models for Semicontinuous Data. The University of Iowa.\n\n\nMonleon, Vicente J, Lisa Madsen, and Lisa C Wilson. 2019. “Small Area Estimation of Zero-Inflated, Spatially Correlated Forest Variables Using Copula Models.” Celebrating Progress, Possibilities, and Partnerships, 93.\n\n\nPizer, Steven D, and Julia C Prentice. 2011. “Time Is Money: Outpatient Waiting Times and Health Insurance Choices of Elderly Veterans in the United States.” Journal of Health Economics 30 (4): 626–36.\n\n\nSimmachan, T, and P Boonkrong. 2024. “A Comparison of Count and Zero-Inflated Regression Models for Predicting Claim Frequencies in Thai Automobile Insurance.” Lobachevskii Journal of Mathematics 45 (12): 6400–6414.\n\n\nSlime, Mekdad, Abdellah Ould Khal, Abdelhak Zoglat, Mohammed El Kamli, and Brahim Batti. 2025. “Optimizing Automobile Insurance Pricing: A Generalized Linear Model Approach to Claim Frequency and Severity.” Statistics, Optimization & Information Computing.\n\n\nSun, Nick. 2020. “Comparison of Gaussian Copula and Random Forests in Zero-Inflated Spatial Prediction for Forestry Applications.”\n\n\nYang, Ming, Joseph E Cavanaugh, and Gideon KD Zamba. 2015. “State-Space Models for Count Time Series with Excess Zeros.” Statistical Modelling 15 (1): 70–90.\n\n\nYoung, Derek S, Eric S Roemmele, and Xuan Shi. 2022. “Zero-Inflated Modeling Part II: Zero-Inflated Models for Complex Data Structures.” Wiley Interdisciplinary Reviews: Computational Statistics 14 (2): e1540.\n\n\nYoung, Derek S, Eric S Roemmele, and Peng Yeh. 2022. “Zero-Inflated Modeling Part i: Traditional Zero-Inflated Count Regression Models, Their Applications, and Computational Tools.” Wiley Interdisciplinary Reviews: Computational Statistics 14 (1): e1541.\n\n\nZhang, Pengcheng, David Pitt, and Xueyuan Wu. 2022. “A New Multivariate Zero-Inflated Hurdle Model with Applications in Automobile Insurance.” ASTIN Bulletin: The Journal of the IAA 52 (2): 393–416.\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On Construction and Estimation of Stationary Mixture Transition Distribution Models.” Journal of Computational and Graphical Statistics 31 (1): 283–93.\n\n\n———. 2023a. “Bayesian Geostatistical Modeling for Discrete-Valued Processes.” Environmetrics 34 (7): e2805.\n\n\n———. 2023b. “Nearest-Neighbor Mixture Models for Non-Gaussian Spatial Processes.” Bayesian Analysis 18 (4): 1191–1222.\n\n\nZhou, Xiaoxiao, Kai Kang, and Xinyuan Song. 2020. “Two-Part Hidden Markov Models for Semicontinuous Longitudinal Data with Nonignorable Missing Covariates.” Statistics in Medicine 39 (13): 1801–16.\n\n\nZou, Yixuan, and Derek S Young. 2024. “Fiducial-Based Statistical Intervals for Zero-Inflated Gamma Data.” Journal of Statistical Theory and Practice 18 (1): 12.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter2/proposed.html",
    "href": "chapter2/proposed.html",
    "title": "11  Proposed Method: Copula-Based Zero-Inflated Gamma MTD Models",
    "section": "",
    "text": "11.1 Copula\nThe proposed model extends the copula-based Gamma MTD model to handle zero-inflated data by reconstructing the marginal distribution. By transforming semi-continuous distributions into continuous distributions, the proposed approach addresses the issues encountered with non-continuity in the copula model, thus maintaining the same effectiveness and flexibility in modeling dependence structures as described in Chapter 4.\nAs stated in Definition \\(\\ref{def:copula}\\), if \\(X_j\\) is continuous for all \\(j\\), then copula function \\(C\\) is unique and differentiable. The joint probability density function of \\(X_j\\), \\(f(x_j)\\), can be factored into the product of the copula density, \\(c\\), and the density of \\(X_j\\), \\(f_j\\), \\(j=1,...,p\\). However, in the case of zero-inflated gamma distribution, \\(X_j\\) is semi-continuous, i.e., it exhibits a point mass at zero combined with a continuous distribution over positive values.\nBuilding on the CE framework for discrete values, zero values are replaced with non-zero values drawn from a continuous uniform distribution. The resulting distribution is continuous, effectively smoothing the zero values while preserving the overall distributional structure, including its dependence structure. We defer the details of the marginal distribution reconstruction to a later section.\nWe use an asterisk (\\(*\\)) to denote that a density is CE-based. Without the asterisk, the notation corresponds to the Gamma MTD model introduced in Part I. Based on a pre-specified stationary marginal density \\(f^*_X\\), we define copulas \\(C^*_l = C_l\\) over continuous random vectors \\((U^*_l, V^*_l)\\), with marginals \\(f_{U^*_l} = f^*_X\\) and \\(f_{V^*_l} = f^*_X\\), analogous to copulas in the Gamma MTD model. For \\(t &gt; L\\), the proposed copula-based zero-inflated Gamma MTD (ZIGamma MTD) model specifies the conditional distribution in the same form as given in \\(\\eqref{eq:cond_distribution_copula}\\) in Chapter 4.\nAs before, a variety of copula families are available, and the proposed model can be readily extended by reconstructing the zero-inflated continuous marginal distribution in a similar manner, provided that the resulting distribution remains within the class of continuous distributions.\nWe consider the Gaussian copula with the dependence parameter \\(\\rho\\), as described in Section 4.1. As previously noted, while the marginal distributions can be arbitrary, they are required to be continuous. In the subsequent section, we outline a technique to transform semi-continuous distributions into continuous distributions.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Proposed Method: Copula-Based Zero-Inflated Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "chapter2/proposed.html#marginal-distribution",
    "href": "chapter2/proposed.html#marginal-distribution",
    "title": "11  Proposed Method: Copula-Based Zero-Inflated Gamma MTD Models",
    "section": "11.2 Marginal Distribution",
    "text": "11.2 Marginal Distribution\nTo construct zero-inflated Gamma for the marginal distribution, the Gamma distribution is first reparametrized in terms of the mean, \\(\\mu\\), and the scale parameter, \\(\\beta\\). Zero values are then replaced with non-zero values drawn from a uniform distribution. Specifically,\n\\[\\begin{equation}\n0 \\leftarrow U_i.  \n\\end{equation}\\]\nwhere \\(U_i\\) follows a continuous uniform distribution on \\((0, \\epsilon)\\) with \\(\\epsilon\\) is a data-driven paramater representing the smallest observed non-zero values. The resulting distribution, denoted as \\(ZIGamma(\\mu, \\beta, P, \\epsilon)\\), is expressed as: \\[\\begin{equation}\nf(x) =\n\\begin{cases}\nUnif(0, \\epsilon) & \\text{with probability } P \\\\\nShiftedGamma(\\mu, \\beta; \\epsilon) & \\text{with probability } 1-P,\n\\end{cases}\n\\end{equation}\\] where \\(\\mu\\) denotes the mean and \\(\\beta\\) the scale parameter of the shifted Gamma distribution, \\(P \\in [0, 1]\\) the zero-inflated probability, and \\(\\epsilon &gt; 0\\) the threshold parameter. The shifted Gamma distribution, \\(ShiftedGamma(\\mu, \\beta; \\epsilon)\\), is a standard Gamma distribution with mean \\(\\mu\\) and scale \\(\\beta\\) that is shifted to the right by \\(\\epsilon\\), with the support \\([\\epsilon, \\infty)\\).\nFigure 11.1 shows the probability density function (PDF) of the zero-inflated gamma distribution, along with the corresponding cumulative distribution function (CDF) for fixing the parameters \\(\\mu = 7\\) and \\(\\beta = 1\\), while varying the parameters \\(\\epsilon\\) and \\(P\\) (\\(\\epsilon = 0.1, 0.4\\); \\(P = 0.1, 0.5, 0.7\\)). Each row of the figure corresponds to a different value of \\(P\\): \\(P = 0.1\\), \\(0.5\\), and \\(0.7\\) from top to bottom. Within each row, plot (a) shows the case for \\(\\epsilon = 0.1\\), with the left and right panels depicting the PDF and CDF, respectively. Plot (b) shows the corresponding curves for \\(\\epsilon = 0.4\\).\nAs also shown in the right panels of both plot (a) and (b) in Figure 11.1, the CDF is continuous and no longer exhibits a point mass at zero, i.e., there is no longer a discontinuous jump at zero. Additionally, the parameter \\(\\epsilon\\) controls the degree of the slope near the origin, where smaller value of \\(\\epsilon\\) results in steep increase in the CDF, while larger value leads to a more gradual increase. Moving to the second row of the plot, both plots in this row correspond to \\(P = 0.5\\), while the final row corresponds to \\(P = 0.7\\). Moreover, as \\(P\\) increases, the contribution of the shifted gamma distribution decreases, as illustrated by the PDF plots on the left panels. In other words, zero values become increasingly dominant over positive values with higher \\(P\\). Additional plots are provided in the Appendix A.\n\n\n\n\n\n\nFigure 11.1: (a), (b): \\(ZIGamma(\\mu = 7, \\beta = 1, P = 0.1, \\epsilon = 0.1, 0.4)\\); (c), (d): \\(ZIGamma(\\mu = 7, \\beta = 1, P = 0.5, \\epsilon = 0.1, 0.4)\\); (e), (f): \\(ZIGamma(\\mu = 7, \\beta = 1, P = 0.7, \\epsilon = 0.1, 0.4)\\). (Left) Probability density function (PDF) and (Right) cumulative distribution function (CDF) of the zero-inflated gamma distribution with varying parameters.\n\n\n\nWhile we use a Gaussian copula with a zero-inflated Gamma marginal distribution to illustrate the structure of the proposed model, the proposed model can be readily extended by reconstructing the marginal distribution similarly. For example, one could construct a Gaussian copula with a zero-inflated log-normal (ZILN) MTD model or a Gumble copula with a zero-inflated Gamma (ZIGamma) MTD model, among other configurations. As copula modeling constitutes a substantial research area beyond the scope of this work, we refer the reader to Joe (2014) for more details.\n\n\n\n\nJoe, Harry. 2014. Dependence Modeling with Copulas. CRC press.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Proposed Method: Copula-Based Zero-Inflated Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "chapter2/computation.html",
    "href": "chapter2/computation.html",
    "title": "12  Overview of MCMC Algorithms",
    "section": "",
    "text": "The full Bayesian model is completed by the specification of prior distributions for the parameters \\(\\mu\\), \\(\\beta\\), \\(P\\), \\(\\epsilon\\), \\(\\boldsymbol{\\rho}\\), and \\(\\boldsymbol{w}\\), where \\(\\mu\\), \\(\\beta\\), \\(P\\) and \\(\\epsilon\\) are parameters of the zero-inflated gamma marginals, and \\(\\boldsymbol{\\rho}\\) and \\(\\boldsymbol{w}\\) are the dependence and weight parameters, respectively. For the copula-based zero-inflated Gamma MTD model, the priors are specified as \\(Gamma(\\mu | u_{\\mu}, v_{\\mu})\\), \\(Gamma(\\beta | u_{\\beta}, v_{\\beta})\\), \\(Unif(P |0, 1)\\), \\(Beta(\\epsilon | 5, 5)\\) scaled to the interval \\([0, 2\\epsilon_0]\\), and \\(Unif(\\rho_l |-1, 1)\\), respectively. For the cdf-based weights, the prior is \\(CDP(\\boldsymbol{w} | \\alpha_0, a_0, b_0)\\).\nThe parameters \\(\\mu\\), \\(\\beta\\), \\(P\\), \\(\\epsilon\\), and \\(\\boldsymbol{\\rho}\\) are updated using a slice sampler (Neal 2003). Following the definition in \\(\\eqref{eq:cond_distribution_copula}\\), denote \\(f_l (x_t | x_{t-l})\\) as \\(f_l (x_t | x_{t-l}) = c_l (x_t, x_{t-l}) f_X(x_t)\\), where \\(f_l\\) is the transition kernel, \\(c_l\\) is the copula density, and \\(f_X\\) is the stationary marginal density. The posterior full conditional distributions for the marginal parameters \\(\\mu\\), \\(\\beta\\), \\(P\\), and \\(\\epsilon\\) are proportional to \\(Gamma(\\mu | u_{\\mu}, v_{\\mu}) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\) and \\(Gamma(\\beta | u_{\\beta}, v_{\\beta})\\) \\(\\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\), \\(Unif(P | 0, 1) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\), and \\(ScaledBeta(5, 5; 0, 2\\epsilon_0) \\prod_{t=L+1}^n f_l (x_t | x_{t-l})\\), respectively. The posterior full conditional distribution for each of the dependence parameters \\(\\boldsymbol{\\rho}\\) is proportional to \\(Unif(\\rho_l |-1, 1) \\prod_{t:z_t = l} c_l (x_t, x_{t-l})\\).\nFor the latent variables \\({\\{z_t\\}}_{t=L+1}^n\\), the posterior full conditional for each \\(z_t\\) is a discrete distribution on \\(\\{1, ..., L\\}\\), where the probability of \\(z_t = l\\), denoted by \\(q_l\\), is proportional to \\(w_l c_l (x_t, x_{t-l})\\), for \\(l = 1,..., L\\). The posterior full conditional distribution for weight parameters \\(\\boldsymbol{w}\\), under the cdf-based prior, is \\(Dirichlet (\\boldsymbol{\\alpha})\\), where \\(\\boldsymbol{\\alpha} = (\\alpha_0 a_1 + M_1, ..., \\alpha_0 a_L + M_L)\\).\nAlgorithm \\(\\ref{alg:mcmc-2}\\) requires data, mtd order, hyperparameters of the priors for \\(\\mu\\), \\(\\beta\\), \\(\\epsilon\\), \\(\\boldsymbol{w}\\), and starting values for \\(\\mu\\), \\(\\beta\\), \\(P\\), \\(\\epsilon\\), \\(\\boldsymbol{\\rho}\\). It also requires tuning parameters for the slice sampler, including step size and upper bounds for \\(\\mu\\), \\(\\beta\\), and \\(\\epsilon\\), along with the general MCMC settings such as number of iterations, burn-in period, and thinning interval. The algorithm outputs posterior samples of \\(\\mu\\), \\(\\beta\\), \\(P\\), \\(\\epsilon\\), \\(\\boldsymbol{\\rho}\\) and \\(\\boldsymbol{w}\\). Asterisk (**) denotes modification of Algorithm \\(\\ref{alg:mcmc}\\).\n\n\n\n\nNeal, Radford M. 2003. “Slice Sampling.” The Annals of Statistics 31 (3): 705–67.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Overview of MCMC Algorithms</span>"
    ]
  },
  {
    "objectID": "chapter2/simulation.html",
    "href": "chapter2/simulation.html",
    "title": "13  Simulation Studies",
    "section": "",
    "text": "13.1 Simulation Settings\nThe goal of simulation studies is to assess accuracy and performance of the proposed model in Chapter 11. We explore a range of configurations by varying the parameters for weight, dependence, and marginal distribution, with particular emphasis on the zero-inflated probability.\nWith weight parameters \\(\\boldsymbol{w}\\), dependence parameters for Gaussian copula \\(\\boldsymbol{\\rho}\\), mean \\(\\mu\\), scale \\(\\beta\\), zero-inflated probability \\(P\\), and threshold parameter \\(\\epsilon\\), we generate \\(n = 2000\\) observations from the copula-based ZIGamma MTD model. For model fitting, we set the order \\(L = 5\\) and consider the Gaussian copula with zero-inflated gamma marginals.\nWe run the Gibbs sampler for \\(165,000\\) iterations, discard the first \\(5000\\) iterations as burn-in, and collect samples every \\(20\\) iterations, resulting in \\(8000\\) iterations per MCMC chain. To ensure that we can assess MCMC convergence and obtain more precise estimates of parameters, we also run four MCMC chains with \\(8000\\) iterations each for all of the following scenarios in Tables (?tbl-scenarios-description-zigamma, ?tbl-scenarios-zigamma), which contain the description and the summary of scenarios, respectively.\nIn all scenarios, we use the cdf-based Dirichlet process (CDP) prior on the weights. Other prior choices, such as the Dirichlet prior and the truncated stick-breaking (SB) prior are readily available, but the original MTD studies has shown that SB and CDP priors give more precise estimates.\nAll scenarios were initially analyzed using a single replicate. Scenarios 1 and 2 were further evaluated with multiple replicates to assess coverage and robustness. Each replicate consisted of a new synthetic dataset generated with the same underlying parameters but different random seeds. Specifically, we ran the models on 10 independently generated replicates for Scenarios 1 and 2 to evaluate the consistency and robustness of the results, ensuring comparability across scenarios.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "chapter2/simulation.html#sec-ch2-simu-res",
    "href": "chapter2/simulation.html#sec-ch2-simu-res",
    "title": "13  Simulation Studies",
    "section": "13.2 Simulation Results",
    "text": "13.2 Simulation Results\n\n13.2.1 Convergence Diagnostics\nScenario 1 in ?tbl-scenarios-zigamma serves as an example to show and track convergence and has the same setup for \\(w\\) and \\(\\rho\\) as Scenario 1 in the original MTD studies.\nTables (?tbl-w-table-s1-zigamma, ?tbl-rho-table-s1-zigamma, ?tbl-mar-table-s1-zigamma) present the posterior estimates and convergence diagnostics for the parameters related to weight, dependence, and marginal distribution, respectively. We defer the discussion of the estimates of the posterior mean and standard deviation (mean and SD) until a later section. There is no evidence of lack of convergence for all parameters (Gelman-Rubin statistic \\(R\\) and its upper CI \\(\\leq 1.1\\)). The simulation error of the estimates is also negligible for all parameters (Naive SE and Time-series SE are close to zero).\nGelman–Rubin convergence diagnostic and ACF plots (Figure C.1, Figure C.2, Figure C.3) can be found in Section C.1.1. In Scenario 1, the chains converge more rapidly for the parameters related to the marginal distribution, achieving convergence at around \\(2000\\) iterations. The chains converge more slowly for the parameters related to weight and dependence, especially at later lags. Nevertheless, all weight and dependence parameters reach convergence by \\(8000\\) iterations. Similar patterns emerge across all other scenarios. Trace and density plots (Figure C.4, Figure C.5, Figure C.6) are also included in Section C.1.1.\n\n\n13.2.2 Weight and Dependence Parameters for Copula\nScenarios 1 and 2 in ?tbl-scenarios-zigamma are employed to demonstrate the effectiveness of weight and dependence construction, as well as their interplay.\nScenario 1 and 2 share the same setup for \\(w\\) and \\(\\rho\\) as the original MTD studies, where weight and dependence are compatible. In Scenario 1, we consider exponentially decreasing weights. In Scenario 2, we consider an uneven arrangement of the relevant lags.\nAs shown in (a), (b) of Figure 13.1, the results appear reasonable; that is, the estimates are consistent with the true values, with minor discrepancies. Nevertheless, the differences are minimal, and the \\(95\\%\\) posterior credible intervals cover the true value for both weight and dependence across all lags.\nConsistent with the results of the Gamma MTD model, placing greater weight on a lag yields narrower \\(95\\%\\) posterior credible interval (CI) for that lag. When less information is available to estimate its influence, the CI widens and approaches the prior distribution.\n\n\n\n\n\n\nFigure 13.1: (a), (b) Results for Scenarios 1 and 2: default setup for \\(w\\) and \\(\\rho\\) (\\(P = 0.1\\) and \\(\\epsilon = 0.1\\)). (Left) Dashed (black) lines are true weights, dot-dashed (red) lines are prior means, solid lines are posterior means, and (purple) polygons are \\(95\\%\\) posterior credible intervals. (Right) Dashed (black) lines are true dependence, dot-dashed (red) lines are prior means, solid (blue) lines are posterior means, and (purple) polygons are \\(95\\%\\) posterior credible intervals.\n\n\n\n\n\n13.2.3 Parameters for Marginal Distributions\nScenario 3 to 6 in ?tbl-scenarios-zigamma are used to evaluate the mean and the scale parameter for the zero-inflated gamma marginal distribution. Scenario 7 through 9 are used to evaluate these parameters in cases with high skewness.\nIn Scenario 3 to 9, we revert to the same settings for weight and dependence as used in Scenario 1. That is, we fix \\(w_i \\propto \\exp(-i), i = 1,..., 5\\) and \\(\\rho_l = (0.7, 0.5, 0.3, 0.1, 0.1)\\).\nFor the ZIGamma MTD model, the slice sampler faces similar challenges when the target distribution is not evaluable. Skewness of the target distribution may reduce sampling efficiency by inducing correlations between successive draws (Planas and Rossi 2024). We explore additional scenarios to identify where the algorithm may fail. Once again, Scenarios 7, 8, and 9 illustrate increasing skewness.\nWe present the results of Scenario 3 and Scenario 7 as examples. Each scenario consists of six cases, covering all combinations of \\(P = 0.1, 0.5, 0.7\\), and \\(\\epsilon = 0.1, 0.4\\). As shown in Tables (?tbl-mar-table-s3-zigamma, ?tbl-mar-table-s7-zigamma), the results appear reasonable; that is, convergence has been achieved and the estimates are consistent with the true values.\nTo demonstrate how plots showing the marginal results for the Gamma MTD model in Section B.1.3 can be generated for the ZIGamma MTD model, we include example plots for Scenario 1 in Section C.1.3: the simulated data in Figure C.7 and the results overlaid on the simulated data in Figure C.8.\n\n\n13.2.4 Coverage Assessment\nTo compute coverage rates, for each of the 10 replicates, we first combine the \\(4\\) chains of \\(8000\\) posterior samples per parameter, then calculate the \\(95\\%\\) credible interval from the combined samples, and record whether the true parameter value falls within this interval. The overall coverage is the proportion of replicates for which the true value is contained within the interval.\nAs shown in Tables (?tbl-coverage-1-zigamma and ?tbl-coverage-2-zigamma), the \\(95\\%\\) credible intervals for all parameters successfully contain the true values in most replicates across both scenarios. Most parameters achieve full coverage, with a few slightly below \\(1\\), indicating that the posterior intervals reliably capture the true parameter values.\n\n\n\n\nPlanas, Christophe, and Alessandro Rossi. 2024. “The Slice Sampler and Centrally Symmetric Distributions.” Monte Carlo Methods and Applications 30 (3): 299–313.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "chapter2/prediction.html",
    "href": "chapter2/prediction.html",
    "title": "14  Prediction",
    "section": "",
    "text": "?tbl-pred-zigamma-s1 and ?tbl-pred-zigamma-s2 summarize the \\(95\\%\\) one-step ahead posterior predictive intervals for Scenario 1 and 2, respectively. The overall coverage can obscure important differences in predictive performance. To provide a clearer picture, we decompose the coverage into below and above. As shown in these tables, when the zero-inflated probability is low (e.g., \\(P = 0.1\\)), the empirical coverage above (i.e., the coverage for data greater than \\(\\epsilon\\)) is a more informative metric for assessing predictive performance. As \\(P\\) increases (e.g., \\(P = 0.5\\), \\(0.7\\)), the empirical coverage below (i.e., the coverage for data less than or equal to \\(\\epsilon\\)) becomes increasingly dominant.\nWhen \\(P\\) is small, a large proportion of observations fall above \\(\\epsilon\\), providing more information to estimate coverage in the upper range. As \\(P\\) increases, more observations concentrate below \\(\\epsilon\\), making the coverage in the near-zero range the primary indicator of the overall performance. This shift reflects the change in the underlying data distribution, where increasing \\(P\\) results in a higher proportion of near-zero values. Similiar patterns are observed across all scenarios considered in the simulation study. Figure 14.1 and Figure 14.2 convey the same findings but present the empirical coverage as time series for Scenario 1 and 2, respectively. Additional plots for Scenario 3 through 9 are provided in the Appendix E.\n\n\n\n\n\n\nFigure 14.1: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 1 (varying \\(P\\) and \\(\\epsilon\\)). Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure 14.2: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 2 (varying \\(P\\) and \\(\\epsilon\\)). Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Prediction</span>"
    ]
  },
  {
    "objectID": "chapter2/discussion.html",
    "href": "chapter2/discussion.html",
    "title": "15  Discussion",
    "section": "",
    "text": "In this work, we review existing models for zero-inflated dependent count and continuous data, as well as the continuous extension approach, and propose a novel copula-based zero-inflated MTD model that extends the existing framework. We also present the algorithms and simulation studies, which demonstrate promising results across various scenarios. The advantage of our proposed approach is that, by reconstructing semi-continuous distributions into continuous forms, it preserves the effectiveness of copula modeling for capturing dependence structures, while retaining flexibility in the selection of marginal distributions in zero-inflated continuous settings.\nIn real-world settings, dependence structures often exhibit features such as tail dependence or asymmetry that cannot be adequately captured by the Gaussian copula. Further research should explore alternative copula families, such as the Clayton or Gumbel copulas, along with efficient estimation techniques and practical applications, to better capture complex dependence patterns in empirical data.\nAlthough the framework could, in principle, be extended to handle non-stationary time series and incorporate predictors, these features are not currently implemented. Consequently, it cannot adequately model non-stationary data with trends and seasonality, limiting its practical effectiveness. In addition, the lack of support for predictors prevents the model from jointly capturing the effects of past observations and relevant predictors. Extending the framework to a non-stationary and regression-based setting would enable it to handle evolving dynamics and integrate both sources of information, thereby enhancing its flexibility and applicability to real-world data.",
    "crumbs": [
      "Models for Forecasting Zero-Inflated Skewed Time Series",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapter3/intro.html",
    "href": "chapter3/intro.html",
    "title": "16  Introduction",
    "section": "",
    "text": "Recurrent Neural Networks (RNNs) (Rumelhart, Hinton, and Williams 1986), and their variants, Long Short-Term Memory (LSTMs), are widely used for modeling sequence data because of their ability to capture both short- and long-term dependencies. In natural language processing, they have been successfully applied to tasks such as handwriting recognition (Graves et al. 2008), language modeling (Mikolov 2012), speech recognition (Chan et al. 2015; Chiu et al. 2017), and machine translation (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014). Beyond language, RNNs and LSTMs have also shown effective in complex time series forecasting and have been employed for applications including financial market prediction (Siami-Namini, Tavakoli, and Namin 2019; Muncharaz 2020; Pirani et al. 2022), energy forecasting (Manero, Béjar, and Cortés 2018; Sandhu, Nair, et al. 2019; Yu, Cao, and Zhu 2019; Paramasivan 2021), weather and climate modeling (Salman et al. 2018; Haq 2022), and epidemiological trend analysis (Chimmula and Zhang 2020; Wang et al. 2020).\nHowever, previous studies comparing LSTMs to traditional models often claim LSTM superiority, a conclusion that can be misleading when the benchmarks chosen are inappropriate. For example, LSTMs are frequently compared to autoregressive integrated moving average (ARIMA) models, even when the assumptions underlying ARIMA models such as stationarity and normally distributed errors are not satisfied (Hewamalage, Ackermann, and Bergmeir 2023). An early survey also reported more nuanced results, showing that RNNs including LSTMs outperform classical benchmarks on some datasets and metrics, but not consistently (Hewamalage, Bergmeir, and Bandara 2021). Similar concerns have also been raised in discussions of newer foundation-model approaches (Bergmeir 2024). These observations highlight the need to evaluate deep learning models against more flexible probabilistic alternatives.\nIn line with this, both probabilistic and deep learning models have been shown to be effective for forecasting univariate time series. For example, a prior study (Hassan 2021) demonstrated that the probabilistic MTD model and the deep learning LSTM network achieved similar predictive accuracy in modeling disease spread, with both slightly outperforming classical ARIMA models. These findings suggest that both probabilistic and deep learning approaches hold promise, yet their relative strengths under varying data conditions remain underexplored in the univariate setting within the statistics and machine learning (ML) literature.\nTo address the concerns about benchmark limitations and to investigate the relative performance of probabilistic and deep learning models, we conduct a rigorous comparison of LSTM and MTD models in the univariate setting. Unlike prior work that benchmarks LSTMs primarily against mis-specified linear models such as ARIMA, we evaluate LSTM against a probabilistic alternative that does not require restrictive assumptions and is better suited to non-linear, non-Gaussian dynamics. Our controlled simulations focus on stationary but non-Gaussian data-generating processes, systematically varying conditions such as marginal skewness, dependency structure, and zero-inflation to assess each model’s strengths and weaknesses. We then complement these simulations with a real-world data application to provide a grounded assessment of practical forecasting performance.\nThe rest of the chapter is organized as follows. We review RNN and LSTM architectures and their foundational concepts, and provide an overview of hyperparameter tuning, training, and evaluation metrics in Chapter 17. Simulation results comparing MTDs and LSTMs are presented in Chapter 18, followed by results from the real-world data application in Chapter 19. Finally, we conclude with a discussion in Chapter 20.\n\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv Preprint arXiv:1409.0473.\n\n\nBergmeir, Christoph. 2024. “LLMs and Foundational Models: Not (yet) as Good as Hoped.” Foresight: The International Journal of Applied Forecasting 73: 33–38.\n\n\nChan, William, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. 2015. “Listen, Attend and Spell.” arXiv Preprint arXiv:1508.01211.\n\n\nChimmula, Vinay Kumar Reddy, and Lei Zhang. 2020. “Time Series Forecasting of COVID-19 Transmission in Canada Using LSTM Networks.” Chaos, Solitons & Fractals 135: 109864.\n\n\nChiu, Chung-Cheng, Dieterich Lawson, Yuping Luo, George Tucker, Kevin Swersky, Ilya Sutskever, and Navdeep Jaitly. 2017. “An Online Sequence-to-Sequence Model for Noisy Speech Recognition.” arXiv Preprint arXiv:1706.06428.\n\n\nGraves, Alex, Marcus Liwicki, Santiago Fernández, Roman Bertolami, Horst Bunke, and Jürgen Schmidhuber. 2008. “A Novel Connectionist System for Unconstrained Handwriting Recognition.” IEEE Transactions on Pattern Analysis and Machine Intelligence 31 (5): 855–68.\n\n\nHaq, Mohd Anul. 2022. “CDLSTM: A Novel Model for Climate Change Forecasting.” Computers, Materials & Continua 71 (2).\n\n\nHassan, Mohamed Yusuf. 2021. “The Deep Learning LSTM and MTD Models Best Predict Acute Respiratory Infection Among Under-Five-Year Old Children in Somaliland.” Symmetry 13 (7): 1156.\n\n\nHewamalage, Hansika, Klaus Ackermann, and Christoph Bergmeir. 2023. “Forecast Evaluation for Data Scientists: Common Pitfalls and Best Practices.” Data Mining and Knowledge Discovery 37 (2): 788–832.\n\n\nHewamalage, Hansika, Christoph Bergmeir, and Kasun Bandara. 2021. “Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions.” International Journal of Forecasting 37 (1): 388–427.\n\n\nManero, Jaume, Javier Béjar, and Ulises Cortés. 2018. “Wind Energy Forecasting with Neural Networks: A Literature Review.” Computación y Sistemas 22 (4): 1085–98.\n\n\nMikolov, Tomáš. 2012. Statistical Language Models Based on Neural Networks. Brno University of Technology.\n\n\nMuncharaz, Javier Oliver. 2020. “Comparing Classic Time Series Models and the LSTM Recurrent Neural Network: An Application to s&p 500 Stocks.” Finance, Markets and Valuation 6 (2): 137–48.\n\n\nParamasivan, Senthil Kumar. 2021. “Deep Learning Based Recurrent Neural Networks to Enhance the Performance of Wind Energy Forecasting: A Review.” Revue d’Intelligence Artificielle 35 (1).\n\n\nPirani, Muskaan, Paurav Thakkar, Pranay Jivrani, Mohammed Husain Bohara, and Dweepna Garg. 2022. “A Comparative Analysis of ARIMA, GRU, LSTM and BiLSTM on Financial Time Series Forecasting.” In 2022 IEEE International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE), 1–6. IEEE.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36.\n\n\nSalman, Afan Galih, Yaya Heryadi, Edi Abdurahman, and Wayan Suparta. 2018. “Single Layer & Multi-Layer Long Short-Term Memory (LSTM) Model with Intermediate Variables for Weather Forecasting.” Procedia Computer Science 135: 89–98.\n\n\nSandhu, KS, Anil Ramachandran Nair, et al. 2019. “A Comparative Study of ARIMA and RNN for Short Term Wind Speed Forecasting.” In 2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT), 1–7. IEEE.\n\n\nSiami-Namini, Sima, Neda Tavakoli, and Akbar Siami Namin. 2019. “A Comparative Analysis of Forecasting Financial Time Series Using Arima, Lstm, and Bilstm.” arXiv Preprint arXiv:1911.09512.\n\n\nSutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to Sequence Learning with Neural Networks.” Advances in Neural Information Processing Systems 27.\n\n\nWang, Peipei, Xinqi Zheng, Gang Ai, Dongya Liu, and Bangren Zhu. 2020. “Time Series Prediction for the Epidemic Trends of COVID-19 Using the Improved LSTM Deep Learning Method: Case Studies in Russia, Peru and Iran.” Chaos, Solitons & Fractals 140: 110214.\n\n\nYu, Yunjun, Junfei Cao, and Jianyong Zhu. 2019. “An LSTM Short-Term Solar Irradiance Forecasting Under Complicated Weather Conditions.” IEEE Access 7: 145651–66.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter3/background.html",
    "href": "chapter3/background.html",
    "title": "17  Background",
    "section": "",
    "text": "17.1 Recurrent Neural Network (RNN) Architecture",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter3/background.html#recurrent-neural-network-rnn-architecture",
    "href": "chapter3/background.html#recurrent-neural-network-rnn-architecture",
    "title": "17  Background",
    "section": "",
    "text": "17.1.1 Recurrent Unit\nAn Recurrent Neural Network (RNN) is composed of repeating cells or units that unfold or unroll over time, where each unit passes recurrent information stored in the hidden state from one time step to the next. Figure 17.1 presents a visual representation of an RNN unit.\n\n\n\n\n\n\nFigure 17.1: Architecture of an RNN unit, reproduced from Olah (2015). \\(x_t\\) is the input, \\(h_t\\) is the hidden state, and \\(o_t\\) is the output. \\(tanh\\) is the activation function, squashing values to \\((-1, 1)\\) for stability and zero-centered output.\n\n\n\nAn RNN unit computes a weighted combination of input data, \\(x_t\\), and the previous hidden state, \\(h_{t-1}\\), applies an activation function, and updates the hidden state to \\(h_t\\). Let \\(x_t\\), \\(h_t\\), and \\(o_t\\) denote the input data, the hidden state, and the output at time \\(t\\), respectively. Then, an RNN unit can be expressed as:\n\\[\\begin{equation}\n\\begin{split}\n\\label{eq:rnn}\nh_t &= f(W_{ih} h_{t-1} + W_{ix} x_t + b_i), \\\\\no_t &= g(W_{oh} \\cdot h_t + b_o),\n\\end{split}\n\\end{equation}\\]\nwhere \\(W_{ix}\\), \\(W_{ih}\\) and \\(W_{oh}\\) denote the weight matrices, and \\(b_i\\) and \\(b_o\\) the bias vectors. The subscripts \\(i\\) and \\(o\\) indicate their steps in RNN: \\(i\\) refer to the input/hidden step (first line in \\(\\eqref{eq:rnn}\\)), and \\(o\\) to the output step (second line in \\(\\eqref{eq:rnn}\\)). \\(f\\) and \\(g\\) denote the activation functions for the hidden layer and output layer, respectively. \\(f\\) is typically set to the logistic sigmoid function, denoted as \\(\\sigma\\), which outputs values in range \\((0, 1)\\) to act as a gate that controls how much information passes through. \\(g\\) is the hyperbolic tangent function, denoted as \\(tanh\\), which outputs values in range \\((-1, 1)\\) to generate output in a stable, zero-centered range.\n\n\n17.1.2 Problems with Long-Term Dependence\nThe RNN unit is prone to the well-documented vanishing gradient issue when processing long sequences (Bengio, Simard, and Frasconi 1994). Gradients can either vanish or explode as they are propagated backward through many time steps. Vanishing gradients occur when gradient values shrink exponentially, making them too small to update the network’s weights. On the other hand, exploding gradients occur when the values grow exponentially, causing excessively large weight updates. Both issues can introduce instability during training and hinder the ability of standard RNNs to capture long-term dependence in sequence data.\nTo capture long-term dependence in sequence data while alleviating the vanishing gradient problem, Hochreiter and Schmidhuber (1997) introduce the Long Short-Term Memory (LSTM) unit. Since this introduction, several LSTM variants have been developed. Notable variants include LSTM with a forget gate (Gers, Schmidhuber, and Cummins 2000), LSTM with peephole connections (Gers and Schmidhuber 2000), and gated recurrent unit (GRU) (Cho et al. 2014).\nOur discussion and experiments focus on the LSTM architecture with a forget gate (Gers, Schmidhuber, and Cummins 2000) for two main reasons. First, this is the LSTM version implemented in PyTorch, a widely used framework for deep learning research and development. Second, while several variants of the vanilla LSTM exist, such as the LSTM with peephole connections (Gers and Schmidhuber 2000) and gated recurrent unit (GRU) (Cho et al. 2014), a comprehensive study has shown that these variants generally offer comparable performance (Greff et al. 2016). For a comprehensive list of vanilla LSTM variants, we refer the reader to Yu et al. (2019) and Hewamalage, Bergmeir, and Bandara (2021).",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter3/background.html#long-short-term-memory-lstm-network-architecture",
    "href": "chapter3/background.html#long-short-term-memory-lstm-network-architecture",
    "title": "17  Background",
    "section": "17.2 Long Short-Term Memory (LSTM) Network Architecture",
    "text": "17.2 Long Short-Term Memory (LSTM) Network Architecture\n\n17.2.1 LSTM Units\nAn Long Short-Term Memory (LSTM) unit extends an RNN by introducing a cell state and three gates: the forget gate, the input gate, and the output gate. The cell state carries long-term dependence, while the hidden state encodes short-term patterns. The gates regulate the flow of information by determining how much of the previous cell state should be forgotten, how much new information should be added, and how much of the updated cell state should be passed to the hidden state at each time step. Figure 17.2 presents a visual representation of an LSTM unit.\n\n\n\n\n\n\nFigure 17.2: Architecture of an LSTM unit with a forget gate, reproduced from Olah (2015). \\(x_t\\) is the input, \\(h_t\\) the hidden state, and \\(c_t\\) the cell state. \\(f_t\\), \\(i_t\\), and \\(o_t\\) are the forget, input, and output gates, respectively. \\(\\sigma\\) is used to squash values to \\((0, 1)\\) for gating, while \\(tanh\\) squashes values to \\((-1, 1)\\) for stability and zero-centered output.\n\n\n\nAn LSTM unit process the input data, \\(x_t\\), and the previous hidden state, \\(h_{t-1}\\), and the cell state, \\(c_{t-1}\\), through several gating mechanisms, updates the cell state to \\(c_t\\) and the hidden state to \\(h_t\\). Let \\(c_t\\) and \\(h_t\\) denote the cell and the hidden state vector. Let \\(f_t\\), \\(i_t\\), and \\(o_t\\) represent the forget, the input, and the output gate vector at time \\(t\\), respectively. Then, an LSTM unit can be expressed as:\n\\[\\begin{equation}\n\\begin{split}\n\\label{eq:lstm}\nf_t &= \\sigma(W_{fh} h_{t-1} + W_{fx} x_t + b_f), \\\\\ni_t &= \\sigma(W_{ih} h_{t-1} + W_{ix} x_t + b_i), \\\\\n\\tilde{c}_t &= \\tanh(W_{\\tilde{c} h} h_{t-1} + W_{\\tilde{c} x} x_t + b_{\\tilde{c}}), \\\\\nc_t &= f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t, \\\\\no_t &= \\sigma(W_{oh} h_{t-1} + W_{ox} x_t + b_o), \\\\\nh_t &= o_t \\cdot \\tanh(c_t),\n\\end{split}\n\\end{equation}\\]\nwhere \\(\\boldsymbol{W}\\) denotes the weight matrices, \\(\\boldsymbol{b}\\) the bias vectors, \\(\\sigma\\) the logistic sigmoid function, and \\(tanh\\) the hyperbolic tangent function.\nThe internal structure of an LSTM unit consists of several components that work together to regulate information flow at each time step \\(t\\):\n\nThe forget gate, \\(f_t\\), controls the extent to which information is discarded or retained. The forget gate outputs values in range \\((0, 1)\\), where \\(0\\) means the information is completely discarded, and \\(1\\) means it is fully retained. The values never reach \\(0\\) or \\(1\\), since the range is exclusive.\nThe input gate, \\(i_t\\), regulates the amount of new information to add. The input gate outputs values in in range \\((0, 1)\\), where \\(0\\) means no information is added, and \\(1\\) means it is nearly fully added. The values never reach \\(0\\) or \\(1\\), since the range is exclusive.\nThe network computes the candidate values, \\(\\tilde{c}\\), which represents the proposed new information.\nNext, the cell state, \\(c_t\\), is updated by combining the previous cell state, \\(c_{t-1}\\), and the candidate values, \\(\\tilde{c}\\). As previously mentioned, \\(f_t\\) controls how much irrelevant information to discard, while \\(i_t\\) determines how much new information to incorporate when updating the cell state.\nThen, the output gate, \\(o_t\\), determines the extent to which the cell state, \\(c_t\\), is exposed to the hidden state, \\(h_t\\).\nThe hidden state, \\(h_t\\), is updated by taking the cell state, \\(c_t\\), and scaling it with the output gate, \\(o_t\\). This resulting hidden state, \\(h_t\\), is the final output of the LSTM network at time \\(t\\).\n\nTo produce the output, \\(\\hat{y}_t\\), a fully connected layer is applied to the hidden state, \\(h_t\\). This layer performs a linear transformation, effectively mapping the high-dimensional representation learned by the LSTM to the target output space. For regression tasks, the output is typically a single scalar representing the prediction, and no non-linear activation is applied, allowing the network to generate an unconstrained real value.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter3/background.html#hyperparameter-tuning-training-and-metrics",
    "href": "chapter3/background.html#hyperparameter-tuning-training-and-metrics",
    "title": "17  Background",
    "section": "17.3 Hyperparameter Tuning, Training, and Metrics",
    "text": "17.3 Hyperparameter Tuning, Training, and Metrics\nHyperparameter tuning plays a crucial role in improving model performance. Key hyperparameters include, for example:\n\nBatch Size\nNumber of Epochs\nLearning Rate\nNumber of Hidden Units or Cell Dimension\nNumber of Hidden Layers, etc.\n\nBatch size refers to the number of training samples or sequences processed simultaneously by the network in one forward and backward pass before updating its parameters. An epoch is one complete pass through the entire training dataset, during which the network processes all batches once, performing one forward and one backward pass per batch. The number of epochs refers the the number of passes the network iterates over the full dataset to achieve optimal training of the RNN.\nThe learning rate controls how much the network’s parameters are adjusted during training in response to the gradients of the loss function. The effectiveness of the learning rate often depends on the optimizer used.\nThe cell dimension and the number of hidden layers and are two additional hyperparameters that define the structure of the RNN architecture. The cell dimension refers to the size of the hidden state vector, which corresponds to the number of neurons or nodes inside each RNN cell. The number of hidden layers determines how many recurrent layers are stacked on top of each other.\nHyperparameter tuning can be performed manually through hand tuning or automatically using methods such as grid search and random search (Bergstra and Bengio 2012). Manual search, also known as manual hyperparameter tuning, involves adjusting hyperparameters based on commonly used defaults, insights from prior literature, and feedback from model performance, while automated tuning involves systematically searching the hyperparameter space. Grid search exhaustively evaluates all possible combinations within a predefined set of hyperparameter values, while random search samples hyperparameter values randomly from specified distributions for evaluation.\nTurning to training, forward pass involves passing input data, \\(x_t\\), through the network to generate a predicted value, \\(\\hat{y}_{t}\\), for each time step from \\(t =1\\) to \\(T\\), as outlined in the steps above in Section 17.2.1. The error is then calculated using a loss function, which measures the discrepancy between the predicted output, \\(\\hat{y}_t\\), and the target value, \\(y_t\\). The total loss is computed by summing up the loss over time:\n\\[\\begin{equation}\n\\begin{split}\n\\mathcal{L}(\\hat{y}, y) = \\sum_{t=1}^T \\ell(\\hat{y}_t, y_t),\n\\end{split}\n\\end{equation}\\]\nwhere \\(\\mathcal{L}\\) represents the overall loss accumulated over time and \\(\\ell_i\\) the loss at each time step \\(t\\).\nBackpropagation involves propagating the error backward through the network, from time step \\(t = T\\) to \\(1\\), and computing the gradients of the objective function with respect to each parameter in the network. These gradients guide how the network parameters should be updated in order to minimize the loss. For sequence-based models, such as RNNs, LSTMs, and GRUs, the Backpropagation Through Time (BPTT) procedure (Paul J. Werbos 1988; P. J. Werbos 1990) is employed as an extension of the standard backpropagation algorithm. BPTT unfolds the network across time steps, allowing the computation of gradients for the entire sequence of inputs. For the specific derivation of LSTM gradients, we refer the reader to Chen (2016) and Sherstinsky (2020).\nOnce the gradients are computed using BPTT, standard gradient-based optimization techniques, such as Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam) (Kingma and Ba 2014), can be used to update the parameters in the direction that minimizes loss. The following update rule reflects the basic form of SGD, where parameters are adjusted using the gradient, scaled by the learning rate:\n\\[\\begin{equation}\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha \\, \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L},\n\\end{equation}\\]\nwhere \\(\\boldsymbol{\\theta} = \\{W_{fx}, W_{ix}, W_{\\tilde{c}x}, W_{ox}, W_{fh}, W_{ih}, W_{\\tilde{c}h}, W_{oh}, \\boldsymbol{b}\\}\\) in \\(\\eqref{eq:lstm}\\) denotes the set of network parameters, with \\(\\boldsymbol{b}\\) representing all bias vectors collectively, \\(\\alpha\\) the learning rate, and \\(\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}\\) the gradient of the loss function with respect to \\(\\theta\\).\nThe process of forward pass, backpropagation, and parameter updates is repeated over multiple epochs until convergence. Convergence is typically determined by stopping criterion such as early stopping based on validation loss, reaching a predefined number of epochs, and when the improvement in loss between epochs falls below a specified threshold (Goodfellow et al. 2016).\nEvaluation metrics are essential for assessing performance and guiding improvements. ?tbl-metrics presents a list of common metrics used for evaluating forecasting models.\nRoot Mean Squared Error (RMSE), like Mean Squared Error (MSE), penalizes outliers, but is more interpretable, since it is expressed in the same units as the target value. Mean Absolute Error (MAE) treats all errors linearly, so it is less sensitive to outliers compared to MSE or RMSE.\nMSE, RMSE, and MAE are scale-dependent metrics. In contrast, Mean Absolute Percentage Error (MAPE), Symmetric MAPE (SMAPE), and Mean Absolute Scaled Error (MASE) are scale-independent, allowing for comparison across datasets with different units. Finally, Mean Absolute Scaled Error (MASE) addresses some of the limitations of MAPE and SMAPE by scaling errors relative to a naive forecast, serving as an additional metric for evaluating forecast accuracy.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter3/background.html#a-note-on-foundation-models-such-as-transformers",
    "href": "chapter3/background.html#a-note-on-foundation-models-such-as-transformers",
    "title": "17  Background",
    "section": "17.4 A Note on Foundation Models such as Transformers",
    "text": "17.4 A Note on Foundation Models such as Transformers\nFoundation models, or large pre-trained models, are general-purpose AI systems trained on large, diverse datasets to learn broad patterns before fine-tuning on specific tasks. This pretraining framework has enabled their widespread adoption across domains. Their rise followed the success of large language models like BERT (Devlin et al. 2019) and GPT-3 (Brown et al. 2020). Typically built on the transformer architecture (Vaswani et al. 2017), these models have excelled in natural language processing (NLP) and computer vision, with extensions to multi-modal and reinforcement learning. Recently, foundation models have been increasingly applied to forecasting tasks, particularly in time series analysis. Notable domain-specific models include TimeGPT-1 (Garza, Challu, and Mergenthaler-Canseco 2024), Lag-Llama (Rasul et al. 2024), TimesFM by Google Research (Das et al. 2024), Tiny Time Mixers by IBM Research (Ekambaram et al. 2024), Moirai by Salesforce (Woo et al. 2024), and Chronos by Amazon (Ansari et al. 2024).\nThese trends highlight the growing preference for transformer architectures in sequence modeling. Unlike RNNs and LSTMs, which process data sequentially via BPTT, transformers leverage multi-head attention with positional encoding to capture dependencies in parallel. Recurrent units are replaced with stacked encoder and decoder layers, each followed by feed-forward neural network layers, resulting in improved training efficiency and stability. For an overview of the transformer architecture and its applications in time series forecasting, see Ahmed et al. (2023). For a comprehensive survey of foundation models, see Liang et al. (2024).\nAs with many deep learning architectures, transformer-based models require large datasets to train effectively and are prone to overfitting, whereas simpler architectures like LSTMs often perform well on smaller datasets, offering easier training and tuning for practical forecasting tasks. Developing robust and interpretable transformer architectures remains challenging, and benchmarking issues persist (Hewamalage, Ackermann, and Bergmeir 2023; Bergmeir 2024). Nonetheless, transformers hold strong potential for advancing areas of ML, including time series forecasting.\n\n\n\n\nAhmed, Sabeen, Ian E Nielsen, Aakash Tripathi, Shamoon Siddiqui, Ravi P Ramachandran, and Ghulam Rasool. 2023. “Transformers in Time-Series Analysis: A Tutorial.” Circuits, Systems, and Signal Processing 42 (12): 7433–66.\n\n\nAnsari, Abdul Fatir, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, et al. 2024. “Chronos: Learning the Language of Time Series.” https://arxiv.org/abs/2403.07815.\n\n\nBengio, Yoshua, Patrice Simard, and Paolo Frasconi. 1994. “Learning Long-Term Dependencies with Gradient Descent Is Difficult.” IEEE Transactions on Neural Networks 5 (2): 157–66.\n\n\nBergmeir, Christoph. 2024. “LLMs and Foundational Models: Not (yet) as Good as Hoped.” Foresight: The International Journal of Applied Forecasting 73.\n\n\nBergstra, James, and Yoshua Bengio. 2012. “Random Search for Hyper-Parameter Optimization.” The Journal of Machine Learning Research 13 (1): 281–305.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nChen, Gang. 2016. “A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation.” arXiv Preprint arXiv:1610.02583.\n\n\nCho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation.” arXiv Preprint arXiv:1406.1078.\n\n\nDas, Abhimanyu, Weihao Kong, Rajat Sen, and Yichen Zhou. 2024. “A Decoder-Only Foundation Model for Time-Series Forecasting.” https://arxiv.org/abs/2310.10688.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86.\n\n\nEkambaram, Vijay, Arindam Jati, Pankaj Dayama, Sumanta Mukherjee, Nam H. Nguyen, Wesley M. Gifford, Chandra Reddy, and Jayant Kalagnanam. 2024. “Tiny Time Mixers (TTMs): Fast Pre-Trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.” https://arxiv.org/abs/2401.03955.\n\n\nGarza, Azul, Cristian Challu, and Max Mergenthaler-Canseco. 2024. “TimeGPT-1.” https://arxiv.org/abs/2310.03589.\n\n\nGers, Felix A, and Jürgen Schmidhuber. 2000. “Recurrent Nets That Time and Count.” In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, 3:189–94. IEEE.\n\n\nGers, Felix A, Jürgen Schmidhuber, and Fred Cummins. 2000. “Learning to Forget: Continual Prediction with LSTM.” Neural Computation 12 (10): 2451–71.\n\n\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep Learning. MIT Press.\n\n\nGreff, Klaus, Rupesh K Srivastava, Jan Koutnı́k, Bas R Steunebrink, and Jürgen Schmidhuber. 2016. “LSTM: A Search Space Odyssey.” IEEE Transactions on Neural Networks and Learning Systems 28 (10): 2222–32.\n\n\nHewamalage, Hansika, Klaus Ackermann, and Christoph Bergmeir. 2023. “Forecast Evaluation for Data Scientists: Common Pitfalls and Best Practices.” Data Mining and Knowledge Discovery 37 (2): 788–832.\n\n\nHewamalage, Hansika, Christoph Bergmeir, and Kasun Bandara. 2021. “Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions.” International Journal of Forecasting 37 (1): 388–427.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80.\n\n\nKingma, Diederik P, and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” arXiv Preprint arXiv:1412.6980.\n\n\nLiang, Yuxuan, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, and Qingsong Wen. 2024. “Foundation Models for Time Series Analysis: A Tutorial and Survey.” In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 6555–65.\n\n\nOlah, Christopher. 2015. “Understanding LSTM Networks.” https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nRasul, Kashif, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, Rishika Bhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, et al. 2024. “Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting.” https://arxiv.org/abs/2310.08278.\n\n\nSherstinsky, Alex. 2020. “Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network.” Physica D: Nonlinear Phenomena 404: 132306.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30.\n\n\nWerbos, P. J. 1990. “Backpropagation Through Time: What It Does and How to Do It.” Proceedings of the IEEE 78 (10): 1550–60. https://doi.org/10.1109/5.58337.\n\n\nWerbos, Paul J. 1988. “Generalization of Backpropagation with Application to a Recurrent Gas Market Model.” Neural Networks 1 (4): 339–56.\n\n\nWoo, Gerald, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. 2024. “Unified Training of Universal Time Series Forecasting Transformers.” https://arxiv.org/abs/2402.02592.\n\n\nYu, Yong, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. 2019. “A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures.” Neural Computation 31 (7): 1235–70.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapter3/simulation.html",
    "href": "chapter3/simulation.html",
    "title": "18  Simulation Studies",
    "section": "",
    "text": "18.1 Network Configuration\nIn this section, we provide an overview of hyperparameter tuning, training process, and evaluation metrics for the LSTM network used in our study.\nOur design choices are informed by the findings of Hewamalage, Bergmeir, and Bandara (2021), which guide the appropriate settings for the LSTM network. The architecture consists of an input layer, followed by one to two LSTM layers, and concludes with a dense layer to balance model complexity and performance.\nThe network is trained using Backpropagation Through Time (BPTT) (Mozer 2013; Robinson and Fallside 1987; Werbos 1988). Although the open-source Cocob (COntinuous COin Betting) optimizer is reported to perform the best, we use the built-in Adam optimizer for its practical convenience and competitive performance. The learning rate is set to \\(0.1\\), \\(0.01\\), and \\(0.001\\), consistent with recommended ranges for Adam. A batch size of 32 and a cell dimension of 64 strike a balance between training efficiency and model capacity, forming the basis of our chosen configuration.\nFinally, model performance is evaluated using RMSE, MAE, MAPE, SMAPE, and MASE, consistent with the metrics discussed in Chapter 17.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "chapter3/simulation.html#experimental-setup",
    "href": "chapter3/simulation.html#experimental-setup",
    "title": "18  Simulation Studies",
    "section": "18.2 Experimental Setup",
    "text": "18.2 Experimental Setup\nThe goals of the simulation studies are threefold: (1) to compare the predictive performance of the LSTM and MTD models, both generally for gamma data and specifically for zero-inflated gamma data, (2) to assess the stability and robustness of their performance, and (3) to investigate the impact of hyperparameters on LSTM performance.\nTo compare the predictive performance of the LSTM and MTD models under various conditions, we run both models on Gamma Scenario 1–9 (see ?tbl-scenarios of Part I for details) and assess their performance using RMSE as the primary evaluation metric. Each model is trained and tested under identical data splits with a ratio of \\(0.8\\) to ensure a fair comparison.\nTo assess the stability and robustness of model performance, we run both the LSTM and MTD models on 10 independently generated replicates of Gamma Scenario 1 (see ?tbl-scenarios of Part I for details). Each replicate consists of a new synthetic dataset generated using the same underlying parameters but with different random seeds. This setup allows us to quantify the variability in model outcomes arising from randomness in data generation and model training, and to evaluate whether the observed performance differences between the LSTM and MTD models are statistically significant.\nTo investigate the impact of hyperparameters on LSTM performance, we run the network with a variety of configurations on Gamma Scenario 1 with 10 independently generated replicates for each configuration. The configurations explore key hyperparameters including the learning rate, the batch size, the number of layers, and the number of hidden units. Specifically, we evaluate the following LSTM configurations:\n\nLearning rate: \\(0.1\\), \\(0.01\\), and \\(0.001\\)\nBatch size: \\(1\\), \\(8\\), \\(16\\), \\(32\\), \\(64\\), and \\(128\\)\nNumber of layers: \\(1\\), \\(2\\), and \\(3\\)\nHidden cell dimensions: \\(32\\), \\(64\\), and \\(128\\)\n\nThis setup allows us to assess the sensitivity of LSTM performance to hyperparameter choices, identify optimial configurations that yield consistent and robust results, and inform the selection of settings for experiments conducted in Chapter 19.\nFor the zero-inflated Gamma settings, we focus exclusively on Scenario 1 (see ?tbl-scenarios-zigamma of Part II for details), since each scenario includes six cases defined by all combinations of \\(P = 0.1, 0.5, 0.7,\\) and \\(\\epsilon = 0.1, 0.4\\), where \\(P\\) represents the zero-inflated probability and \\(\\epsilon\\) denotes the threshold value. We similarly run both models and evaluate their performance using RMSE, allowing us to specifically examine model behavior on zero-inflated data. Additionally, we compute RMSE below (i.e., RMSE for data greater than \\(\\epsilon\\)) and above (i.e., RMSE for data greater than \\(\\epsilon\\)) to assess predictive accuracy in the lower and upper ranges, respectively. As discussed in Chapter 14, when \\(P\\) is small, a large proportion of observations fall above \\(\\epsilon\\), providing more information to estimate the value in the upper range. As \\(P\\) increases, more observations concentrate below \\(\\epsilon\\), making the RMSE in the near-zero range the primary indicator of the overall performance. These additional metrics provide insight into model performance for low and high-value regions, particularly relevant in the context of zero-inflated distributions.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "chapter3/simulation.html#sec-ch3-simu-res",
    "href": "chapter3/simulation.html#sec-ch3-simu-res",
    "title": "18  Simulation Studies",
    "section": "18.3 Results",
    "text": "18.3 Results\n\n18.3.1 Prediction for Gamma Scenarios\nAs discussed in ?tbl-scenarios-description of Part I, Scenarios 1 and 2 follow the original MTD setup: Scenario 1 uses exponentially decreasing weights, which are typically observed in real-world data, and Scenario 2 uses unevenly arranged relevant lags. Scenarios 3 to 9 follow the same weight pattern as Scenario 1. Scenarios 3 to 6 evaluate gamma shape and rate, and Scenarios 7 to 9 consider high-skew cases.\n?tbl-pred-gamma-lstm-vs-mtd summarizes the RMSE comparisons between LSTM and MTD based on one-step ahead predicted means for Scenarios 1 through 9. The predicted results from LSTM and MTD are similar. RMSEs for MTD are lower in Scenarios 2, 3, 4, and 6, though the differences are minimal. Conversely, LSTM yields slightly lower RMSEs in Scenarios 7 to 9, though the differences are again minimal. RMSEs are the highest for both models in Scenario 2. ?tbl-pred-gamma-lstm-vs-mtd-bias presents the corresponding bias comparisons across the same scenarios. Overall, biases are small, with positive values indicating overestimation and negative values underestimation.\nFigure 18.1 illustrates these means for Scenario 1 and 2. Figure 18.2 presents a zoomed-in view of the same plot, focusing on a subset of the test data (\\(n = 200\\)). As shown in Plot (a), both models predict well, with LSTM performing comparably to MTD. However, as shown in Plot (b), both models appear to struggle more in Scenario 2 compared to their performance in Scenario 1. Additional plots illustrating the predicted means for Scenarios 3 through 9 (Figure F.1, Figure F.2) are provided in the Section F.1.\n\n\n\n\n\n\nFigure 18.1: One-step ahead predicted means for Gamma Scenario 1 and 2: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means.\n\n\n\n\n\n\n\n\n\nFigure 18.2: Zoomed-in view of one-step ahead predicted means for Gamma Scenario 1 and 2: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means.\n\n\n\nUsing Scenario 1 with 10 replicates, we conduct additional analyses to evaluate model performance and assess whether the performance differences between LSTM and MTD are significant. Results from the paired t-test indicated a mean difference in RMSE of \\(0.128957\\) (p-value = 0.005175, df = 9), with MTD consistently yielding lower RMSEs. Figure 18.3 illustrates these findings.\n\n\n\n\n\n\nFigure 18.3: Relative Performance of LSTM and MTD models for Gamma Scenario 1.\n\n\n\nReusing Scenario 1 with 10 replicates, we perform additional analyses to determine whether hyperparameter tuning is necessary. For each hyperparameter, we conduct a repeated-measures ANOVA, treating hyperparameter levels as the treatment factor and replicate ID, representing different simulated data replicates, as the random effect. If the overall p-value is smaller than \\(0.05\\), we follow up with the Bonferroni-corrected pairwise comparisons to identify which pairs differ significantly.\nAmong these configurations, the p-value is statistically significant for batch size (Pr(&gt;F) = 4.4e-06, df = 2, 24), and pairwise comparisons indicate that RMSEs differ significantly only between batch size \\(64\\) and all other batch sizes (\\(1, 8, 16, 32,\\) and \\(128\\)), as well as between batch size 128 and all other batch sizes (\\(1, 8, 16, 32,\\) and \\(64\\)). The p-value is also significant for cell dimensions (Pr(&gt;F) = 0.0113, df = 2, 24); however, pairwise comparisons reveal significant differences in RMSE only between cell dimensions of \\(32\\) and \\(64\\), as well as between \\(32\\) and \\(128\\), but not between \\(64\\) and \\(128\\). Figure 18.4 illustrates these findings.\nThese results indicate that further hyperparameter tuning yields minimal performance gains. Notably, reducing the batch size slows down model training, although the model still completes within minutes, but does not produce a practical improvement in RMSE. Therefore, we adopt the default configuration for subsequent experiments: learning rate = \\(0.001\\), batch size = \\(32\\), number of layers = \\(1\\), and cell dimension = \\(64\\).\n\n\n\n\n\n\nFigure 18.4: Relative performance of LSTM networks with varying learning rates (0.1, 0.01, 0.001), batch sizes (1, 8, 16, 32, 64, 128), number of layers (1–3), and cell dimensions (32, 64, 128) for Gamma Scenario 1.\n\n\n\n\n\n18.3.2 Prediction for Zero-inflated Gamma Scenarios\n?tbl-pred-zigamma-lstm-vs-mtd summarizes the RMSE comparisons between LSTM and MTD based on one-step ahead predicted means for Scenarios 1, with rows correspond to all combinations of \\(Pi = 0.1, 0.5, 0.7,\\) and \\(\\epsilon = 0.1, 0.4\\), where \\(P\\) is the zero-inflated probability and \\(\\epsilon\\) is the threshold value. LSTM generally achieves lower overall RMSEs compared to MTD.\nHowever, patterns similar to those in Chapter 14 of Part II reappear. The overall RMSE can obscure important differences in predictive performance. To provide a clearer picture, we decompose the RMSE into below and above in ?tbl-pred-zigamma-lstm-vs-mtd-lu. Specifically, for zero-inflated gamma data with low zero-inflation probability (e.g., \\(P = 0.1\\)), the RMSE above (which reflects predictive accuracy for values exceeding \\(\\epsilon\\)) is a more informative measure of performance. As \\(P\\) increases, this relationship reverses, and the RMSE below (which captures accuracy on values less than or equal to \\(\\epsilon\\)) becomes more relevant. As shown in ?tbl-pred-zigamma-lstm-vs-mtd-lu, when \\(P = 0.1\\), MTD outperforms LSTM in RMSE above. This trend persists at higher levels of zero-inflation (e.g., \\(P = 0.5\\), \\(0.7\\)), where MTD again yields lower values for RMSE below than LSTM. ?tbl-pred-zigamma-lstm-vs-mtd-bias presents the corresponding bias comparisons across the same scenario. Similar pattern emerges.\nFigure 18.5 illustrates these patterns for Scenario 1. Figure 18.6 presents a zoomed-in view of the same plot, focusing on a subset of the test data (\\(n = 200\\)). Results for Scenario 2 (?tbl-pred-zigamma-lstm-vs-mtd-s2, ?tbl-pred-zigamma-lstm-vs-mtd-s2-lu, ?tbl-pred-zigamma-lstm-vs-mtd-s2-bias, ?tbl-pred-zigamma-lstm-vs-mtd-s2-lu-bias, Figure F.3, Figure F.4) are provided in Section F.2 and are similar to those in Scenario 1.\n\n\n\n\n\n\nFigure 18.5: One-step ahead predicted means for ZIGamma Scenario 1: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means. Reported values show overall RMSE, with decomposed RMSE below and above the threshold shown in parentheses: RMSE (RMSE for data \\(\\leq \\epsilon\\), RMSE for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure 18.6: Zoomed-in view of one-step ahead predicted means for ZIGamma Scenario 1: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means. Reported values show overall RMSE, with decomposed RMSE below and above the threshold shown in parentheses: RMSE (RMSE for data \\(\\leq \\epsilon\\), RMSE for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\nHewamalage, Hansika, Christoph Bergmeir, and Kasun Bandara. 2021. “Recurrent Neural Networks for Time Series Forecasting: Current Status and Future Directions.” International Journal of Forecasting 37 (1): 388–427.\n\n\nMozer, Michael C. 2013. “A Focused Backpropagation Algorithm for Temporal Pattern Recognition.” In Backpropagation, 137–69. Psychology Press.\n\n\nRobinson, Anthony J, and Frank Fallside. 1987. The Utility Driven Dynamic Error Propagation Network. Vol. 11. University of Cambridge Department of Engineering Cambridge.\n\n\nWerbos, Paul J. 1988. “Generalization of Backpropagation with Application to a Recurrent Gas Market Model.” Neural Networks 1 (4): 339–56.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simulation Studies</span>"
    ]
  },
  {
    "objectID": "chapter3/real.html",
    "href": "chapter3/real.html",
    "title": "19  Data Applications: NASA MERRA-2 Wind Speeds Data",
    "section": "",
    "text": "19.1 Experimental Setup",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Applications: NASA MERRA-2 Wind Speeds Data</span>"
    ]
  },
  {
    "objectID": "chapter3/real.html#experimental-setup",
    "href": "chapter3/real.html#experimental-setup",
    "title": "19  Data Applications: NASA MERRA-2 Wind Speeds Data",
    "section": "",
    "text": "19.1.1 Data Access and Description\nThe MERRA-2 wind speed data is accessed and downloaded from the NASA GES DISC Earthdata API, and subsequently processed to extract and interpolate wind speed components at multiple heights (50 m, 10 m, 2 m) for the Limon Wind Energy Center, the largest wind farm in Colorado. Figure 19.1 shows the time series of wind speeds (m/s) at these heights for 2024.\nThe datasets used for the experiments are:\n\nwind speeds (m/s) at heights of 50 m above ground level\nwind speeds (m/s) at heights of 10 m above ground level\nwind speeds (m/s) at heights of 2 m above ground level\n\n\n\n\n\n\n\nFigure 19.1: Time series plot of observed wind speeds (m/s) at heights of (a) 50 m, (b) 10 m, and (c) 2 m above ground level at the Limon Wind Energy Center, Colorado, for the year 2024. Data sourced from MERRA-2 via the NASA GES DISC Earthdata API.\n\n\n\n\n\n19.1.2 Model Configuration and Implementation\nFor MTD, the hyperparameter settings are detailed in Chapter 5. MTD is implemented in R and executed on a high-performance computing (HPC) cluster. Training is performed following the procedure described in Algorithm \\(\\ref{alg:mcmc}\\).\nFor LSTM, we reuse the default configuration for subsequent experiments: learning rate = \\(0.001\\), batch size = \\(32\\), number of layers = \\(1\\), and cell dimension = \\(64\\). LSTM is implemented in PyTorch and trained on a standard workstation.\nDuring training, each iteration of the LSTM loop involves:\n\nPerform a forward pass to obtain predictions.\nCalculate the loss between predictions and targets.\nCompute gradients of the loss with respect to network parameters via backpropagation.\nUpdate the parameters using the Adam optimizer.\nReturn the training loss.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Applications: NASA MERRA-2 Wind Speeds Data</span>"
    ]
  },
  {
    "objectID": "chapter3/real.html#results",
    "href": "chapter3/real.html#results",
    "title": "19  Data Applications: NASA MERRA-2 Wind Speeds Data",
    "section": "19.2 Results",
    "text": "19.2 Results\n\n19.2.1 Prediction Results\nIn the initial configuration, the LSTM network are trained using an L2 loss function and evaluated against the MTD model using RMSE. For MAE evaluation, a separate LSTM is trained with an L1 loss function. In contrast, MTD does not rely on training with an explicit loss function.\nTo facilitate a more rigorous and independent comparison, we incorporate additional error metrics, namely MAPE, SMAPE, and MASE to evaluate and compare the performance of the LSTM and MTD models. All LSTM models are trained using an L2 loss function, except for the one evaluated with MAE, which is trained using an L1 loss function.\nAs shown in Tables (?tbl-real-uv50, ?tbl-real-uv10, ?tbl-real-uv2), MTD consistently outperforms the LSTM models across all evaluated metrics and training configurations for wind speeds at 50 m, 10 m, and 2 m above ground level. Since all reported MASE values are less than 1, this indicates that both models outperform the naïve forecasting benchmark on average.\nTo strengthen the comparison, we conduct an additional experiment by increasing the LSTM input window size, \\(W\\), and the MTD order, \\(L\\), from \\(5\\) to \\(15\\) (i.e., longer look-back steps). We then re-evaluate these models using RMSE, MAE, MAPE, and SMAPE.\nAs shown in Tables (?tbl-real-uv50-L15, ?tbl-real-uv10-L15, ?tbl-real-uv2-L15), the performance of the LSTM network shows slight improvement with a larger window size, though the gains are minimal. In contrast, the MTD model shows no performance gain, but still consistently outperform the LSTM across all metrics.\nGiven that batch size appears to be an important hyperparameter for LSTM, we further test values of \\(8\\), \\(16\\), \\(64\\), \\(128\\), and \\(256\\), compared to the baseline of \\(32\\). Nevertheless, none of these settings yield better performance than the MTD model. Corresponding training and validation loss plots are provided in the Section F.3.\nFigure 19.2 illustrates the one-step-ahead predicted mean wind speeds (in m/s) between LSTM and MTD models at heights of 50 m, 10 m, and 2 m above ground level. Figure 19.3 presents a zoomed-in view of the same plot, focusing on a subset of the test data (\\(n = 200\\)). Prediction error plots (Figure F.5, Figure F.6) are provided in Section F.3.\n\n\n\n\n\n\nFigure 19.2: One-step ahead predicted means for wind speeds (m/s) at heights of (a) 50 m, (b) 10 m, and (c) 2 m above ground level: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means.\n\n\n\n\n\n\n\n\n\nFigure 19.3: Zoomed-in view of one-step ahead predicted means for wind speeds (m/s) at (a) 50 m, (b) 10 m, and (c) 2 m above ground level for \\(n = 200\\). Solid (black) lines represent true values. Dashed (red) lines are LSTM predictions; dashed (blue) lines are MTD predictions.\n\n\n\n\n\n19.2.2 Empirical Coverage of the MTD Model\nEmpirical coverage is particularly relevant for probabilistic forecasting methods such as MTD, where uncertainty estimation is an integral part of the model output. Therefore, in addition to standard evaluation metrics, MTD is also assessed using this technique to evaluate the reliability of its predictive intervals.\n?tbl-pred-mtd-real summaries the \\(95\\%\\) one-step ahead posterior predictive intervals for wind speeds (m/s) at heights of 50 m, 10 m, and 2 m above ground level. As shown in the table, the model appropriately captures the predictive uncertainty across wind speeds at all heights. Figure 19.4 and Figure 19.5 illustrate these intervals.\n\n\n\n\n\n\nFigure 19.4: \\(95\\%\\) one-step ahead posterior predictive intervals for wind speeds (m/s) at heights of (a) 50 m, (b) 10 m, and (c) 2 m above ground level for look-back steps \\(L = 5\\).\n\n\n\n\n\n\n\n\n\nFigure 19.5: \\(95\\%\\) one-step ahead posterior predictive intervals for wind speeds (m/s) at heights of (a) 50 m, (b) 10 m, and (c) 2 m above ground level for look-back steps \\(L = 15\\).",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data Applications: NASA MERRA-2 Wind Speeds Data</span>"
    ]
  },
  {
    "objectID": "chapter3/discussion.html",
    "href": "chapter3/discussion.html",
    "title": "20  Discussion",
    "section": "",
    "text": "In this work, we review LSTMs and evaluate their performance relative to our proposed MTD models. Through simulation studies, we compare both the Gamma MTD and ZIGamma MTD models against the LSTMs. For gamma time series, Gamma MTD and LSTM perform comparably. In contrast, for zero-inflated gamma time series, ZIGamma MTD outperforms the LSTM, which is expected given the challenges of modeling zero-inflated data, and LSTMs are less specialized for handling this type of data.\nIn real-world data applications, we focus on comparing the Gamma MTD with LSTMs to assess their practical performance. In this case, Gamma MTD consistently outperforms the LSTM across all evaluation metrics for all three datasets, including wind speed measurements (m/s) at heights of 50 m, 10 m, and 2 m above ground level.\nProbabilistic models like MTDs offer greater robustness and interpretability due to their probabilistic nature, allowing uncertainty quantification and insights into temporal dependencies. However, MTD models such as Gamma MTD and ZIGamma MTD require careful design and specification. In contrast, deep learning networks such as LSTMs are more general-purpose and provide faster computation, though their black-box structure limits interpretability. Therefore, MTD is better suited for explainable and robust modeling, while LSTMs are advantageous for large-scale or computationally demanding tasks.\nGiven the growing preference for transformer architectures in sequence modeling, future research should extend these comparisons to include transformer-based models. It is equally important to ensure that these comparisons are grounded in appropriate benchmarks and evaluated with suitable metrics, ensuring that conclusions regarding model performance are fair, valid, and contextually appropriate.",
    "crumbs": [
      "Copula-Based Markov MTD Models vs. Deep Learning LSTM Networks",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Abraham, Zubin, and Pang-Ning Tan. 2009. “A Semi-Supervised\nFramework for Simultaneous Classification and Regression of\nZero-Inflated Time Series Data with Application to Precipitation\nPrediction.” In 2009 IEEE International Conference on Data\nMining Workshops, 644–49. IEEE.\n\n\nAhmed, Sabeen, Ian E Nielsen, Aakash Tripathi, Shamoon Siddiqui, Ravi P\nRamachandran, and Ghulam Rasool. 2023. “Transformers in\nTime-Series Analysis: A Tutorial.” Circuits, Systems, and\nSignal Processing 42 (12): 7433–66.\n\n\nAlqawba, Mohammed, and Norou Diawara. 2021. “Copula-Based Markov\nZero-Inflated Count Time Series Models with Application.”\nJournal of Applied Statistics 48 (5): 786–803.\n\n\nAlqawba, Mohammed, Norou Diawara, and N Rao Chaganty. 2019.\n“Zero-Inflated Count Time Series Models Using Gaussian\nCopula.” Sequential Analysis 38 (3): 342–57.\n\n\nAl-Wahsh, H, and A Hussein. 2019. “Estimation of Zero-Inflated\nParameter-Driven Models via Data Cloning.” Journal of\nStatistical Computation and Simulation 89 (6): 951–65.\n\n\nAnsari, Abdul Fatir, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro\nMercado, Huibin Shen, Oleksandr Shchur, et al. 2024. “Chronos:\nLearning the Language of Time Series.” https://arxiv.org/abs/2403.07815.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural\nMachine Translation by Jointly Learning to Align and Translate.”\narXiv Preprint arXiv:1409.0473.\n\n\nBartolucci, Francesco, and Alessio Farcomeni. 2010. “A Note on the\nMixture Transition Distribution and Hidden Markov Models.”\nJournal of Time Series Analysis 31 (2): 132–38.\n\n\nBengio, Yoshua, Patrice Simard, and Paolo Frasconi. 1994.\n“Learning Long-Term Dependencies with Gradient Descent Is\nDifficult.” IEEE Transactions on Neural Networks 5 (2):\n157–66.\n\n\nBerchtold, André. 2001. “Estimation in the Mixture Transition\nDistribution Model.” Journal of Time Series Analysis 22\n(4): 379–97.\n\n\nBerchtold, André, and Adrian Raftery. 2002. “The Mixture\nTransition Distribution Model for High-Order Markov Chains and\nNon-Gaussian Time Series.” Statistical Science 17 (3):\n328–56.\n\n\nBergmeir, Christoph. 2024a. “LLMs and Foundational Models: Not\n(yet) as Good as Hoped.” Foresight: The International Journal\nof Applied Forecasting 73: 33–38.\n\n\n———. 2024b. “LLMs and Foundational Models: Not (yet) as Good as\nHoped.” Foresight: The International Journal of Applied\nForecasting 73.\n\n\nBergstra, James, and Yoshua Bengio. 2012. “Random Search for\nHyper-Parameter Optimization.” The Journal of Machine\nLearning Research 13 (1): 281–305.\n\n\nBermúdez, Lluı́s, and Dimitris Karlis. 2022. “Copula-Based\nBivariate Finite Mixture Regression Models with an Application for\nInsurance Claim Count Data.” TEST 31 (4): 1082–99.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot Learners.” Advances in Neural Information\nProcessing Systems 33: 1877–1901.\n\n\nChan, William, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. 2015.\n“Listen, Attend and Spell.” arXiv Preprint\narXiv:1508.01211.\n\n\nChatterjee, Saptarshi, Shrabanti Chowdhury, Himel Mallick, Prithish\nBanerjee, and Broti Garai. 2018. “Group Regularization for\nZero-Inflated Negative Binomial Regression Models with an Application to\nHealth Care Demand in Germany.” Statistics in Medicine\n37 (20): 3012–26.\n\n\nChen, Gang. 2016. “A Gentle Tutorial of Recurrent Neural Network\nwith Error Backpropagation.” arXiv Preprint\narXiv:1610.02583.\n\n\nChimmula, Vinay Kumar Reddy, and Lei Zhang. 2020. “Time Series\nForecasting of COVID-19 Transmission in Canada Using LSTM\nNetworks.” Chaos, Solitons & Fractals 135: 109864.\n\n\nChiu, Chung-Cheng, Dieterich Lawson, Yuping Luo, George Tucker, Kevin\nSwersky, Ilya Sutskever, and Navdeep Jaitly. 2017. “An Online\nSequence-to-Sequence Model for Noisy Speech Recognition.”\narXiv Preprint arXiv:1706.06428.\n\n\nCho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning\nPhrase Representations Using RNN Encoder-Decoder for Statistical Machine\nTranslation.” arXiv Preprint arXiv:1406.1078.\n\n\nChowdhury, Shrabanti, Saptarshi Chatterjee, Himel Mallick, Prithish\nBanerjee, and Broti Garai. 2019. “Group Regularization for\nZero-Inflated Poisson Regression Models with an Application to Insurance\nRatemaking.” Journal of Applied Statistics 46 (9):\n1567–81.\n\n\nD’Amico, Guglielmo, Riccardo De Blasis, and Filippo Petroni. 2023.\n“The Mixture Transition Distribution Approach to Networks:\nEvidence from Stock Markets.” Physica A: Statistical\nMechanics and Its Applications 632: 129335.\n\n\nDas, Abhimanyu, Weihao Kong, Rajat Sen, and Yichen Zhou. 2024. “A\nDecoder-Only Foundation Model for Time-Series Forecasting.” https://arxiv.org/abs/2310.10688.\n\n\nDenuit, Michel, and Philippe Lambert. 2005. “Constraints on\nConcordance Measures in Bivariate Discrete Data.” Journal of\nMultivariate Analysis 93 (1): 40–57.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“Bert: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding.” In Proceedings of the 2019\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 4171–86.\n\n\nDong, Chunjiao, Stephen H Richards, David B Clarke, Xuemei Zhou, and\nZhuanglin Ma. 2014. “Examining Signalized Intersection Crash\nFrequency Using Multivariate Zero-Inflated Poisson Regression.”\nSafety Science 70: 63–69.\n\n\nDuan, Naihua, Willard G Manning, Carl N Morris, and Joseph P Newhouse.\n1983. “A Comparison of Alternative Models for the Demand for\nMedical Care.” Journal of Business & Economic\nStatistics 1 (2): 115–26.\n\n\nDzupire, Nelson Christopher, Philip Ngare, and Leo Odongo. 2018.\n“A Poisson-Gamma Model for Zero Inflated Rainfall Data.”\nJournal of Probability and Statistics 2018 (1): 1012647.\n\n\nEkambaram, Vijay, Arindam Jati, Pankaj Dayama, Sumanta Mukherjee, Nam H.\nNguyen, Wesley M. Gifford, Chandra Reddy, and Jayant Kalagnanam. 2024.\n“Tiny Time Mixers (TTMs): Fast Pre-Trained Models for Enhanced\nZero/Few-Shot Forecasting of Multivariate Time Series.” https://arxiv.org/abs/2401.03955.\n\n\nFeng, Tianshu. 2020. Zero-Inflated Models for Semi-Continuous\nTransportation Data. University of Washington.\n\n\nGarza, Azul, Cristian Challu, and Max Mergenthaler-Canseco. 2024.\n“TimeGPT-1.” https://arxiv.org/abs/2310.03589.\n\n\nGenest, Christian, and Johanna Nešlehová. 2007. “A Primer on\nCopulas for Count Data.” ASTIN Bulletin: The Journal of the\nIAA 37 (2): 475–515.\n\n\nGers, Felix A, and Jürgen Schmidhuber. 2000. “Recurrent Nets That\nTime and Count.” In Proceedings of the IEEE-INNS-ENNS\nInternational Joint Conference on Neural Networks. IJCNN 2000. Neural\nComputing: New Challenges and Perspectives for the New Millennium,\n3:189–94. IEEE.\n\n\nGers, Felix A, Jürgen Schmidhuber, and Fred Cummins. 2000.\n“Learning to Forget: Continual Prediction with LSTM.”\nNeural Computation 12 (10): 2451–71.\n\n\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.\n2016. Deep Learning. MIT Press.\n\n\nGraves, Alex, Marcus Liwicki, Santiago Fernández, Roman Bertolami, Horst\nBunke, and Jürgen Schmidhuber. 2008. “A Novel Connectionist System\nfor Unconstrained Handwriting Recognition.” IEEE Transactions\non Pattern Analysis and Machine Intelligence 31 (5): 855–68.\n\n\nGreff, Klaus, Rupesh K Srivastava, Jan Koutnı́k, Bas R Steunebrink, and\nJürgen Schmidhuber. 2016. “LSTM: A Search Space Odyssey.”\nIEEE Transactions on Neural Networks and Learning Systems 28\n(10): 2222–32.\n\n\nHao, Wang, Yang Ya-dong, and Ma Yong. 2016. “Research on the\nYangtze River Accident Casualties Using Zero-Inflated Negative Binomial\nRegression Technique.” In 2016 IEEE International Conference\non Intelligent Transportation Engineering (ICITE), 72–75. IEEE.\n\n\nHaq, Mohd Anul. 2022. “CDLSTM: A Novel Model for Climate Change\nForecasting.” Computers, Materials & Continua 71\n(2).\n\n\nHassan, Mohamed Yusuf. 2021. “The Deep Learning LSTM and MTD\nModels Best Predict Acute Respiratory Infection Among Under-Five-Year\nOld Children in Somaliland.” Symmetry 13 (7): 1156.\n\n\nHewamalage, Hansika, Klaus Ackermann, and Christoph Bergmeir. 2023a.\n“Forecast Evaluation for Data Scientists: Common Pitfalls and Best\nPractices.” Data Mining and Knowledge Discovery 37 (2):\n788–832.\n\n\n———. 2023b. “Forecast Evaluation for Data Scientists: Common\nPitfalls and Best Practices.” Data Mining and Knowledge\nDiscovery 37 (2): 788–832.\n\n\nHewamalage, Hansika, Christoph Bergmeir, and Kasun Bandara. 2021.\n“Recurrent Neural Networks for Time Series Forecasting: Current\nStatus and Future Directions.” International Journal of\nForecasting 37 (1): 388–427.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term\nMemory.” Neural Computation 9 (8): 1735–80.\n\n\nHyndman, Rob J, and Gary K Grunwald. 2000. “Applications:\nGeneralized Additive Modelling of Mixed Distribution Markov Models with\nApplication to Melbourne’s Rainfall.” Australian & New\nZealand Journal of Statistics 42 (2): 145–58.\n\n\nJiang, Yonglei, Adolf KY Ng, Yunpeng Wang, Lu Wang, and Bin Yu. 2018.\n“Locational Characteristics of Firms in the Business Service\nIndustry in Airport Economic Zones: Case of Shanghai Hongqiao\nInternational Airport.” Journal of Urban Planning and\nDevelopment 144 (1): 04018001.\n\n\nJoe, Harry. 2014. Dependence Modeling with Copulas. CRC press.\n\n\nJordan, Michael I. 2004. “Graphical Models.”\nStatistical Science 19 (1): 140–55.\n\n\nKaewprasert, Theerapong, Sa-Aat Niwitpong, and Suparat Niwitpong. 2022.\n“Simultaneous Confidence Intervals for the Ratios of the Means of\nZero-Inflated Gamma Distributions and Its Application.”\nMathematics 10 (24): 4724.\n\n\n———. 2024. “Bayesian Confidence Intervals for the Ratio of the\nMeans of Zero-Inflated Gamma Distributions with Application to Rainfall\nData.” Communications in Statistics-Simulation and\nComputation 53 (12): 5780–96.\n\n\nKingma, Diederik P, and Jimmy Ba. 2014. “Adam: A Method for\nStochastic Optimization.” arXiv Preprint\narXiv:1412.6980.\n\n\nLambert, Diane. 1992. “Zero-Inflated Poisson Regression, with an\nApplication to Defects in Manufacturing.” Technometrics\n34 (1): 1–14.\n\n\nLe, Nhu D, R Douglas Martin, and Adrian Raftery. 1996. “Modeling\nFlat Stretches, Bursts Outliers in Time Series Using Mixture Transition\nDistribution Models.” Journal of the American Statistical\nAssociation 91 (436): 1504–15.\n\n\nLele, Subhash R, Brian Dennis, and Frithjof Lutscher. 2007. “Data\nCloning: Easy Maximum Likelihood Estimation for Complex Ecological\nModels Using Bayesian Markov Chain Monte Carlo Methods.”\nEcology Letters 10 (7): 551–63.\n\n\nLele, Subhash R, Khurram Nadeem, and Byron Schmuland. 2010.\n“Estimability and Likelihood Inference for Generalized Linear\nMixed Models Using Data Cloning.” Journal of the American\nStatistical Association 105 (492): 1617–25.\n\n\nLiang, Yuxuan, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin\nSong, Shirui Pan, and Qingsong Wen. 2024. “Foundation Models for\nTime Series Analysis: A Tutorial and Survey.” In Proceedings\nof the 30th ACM SIGKDD Conference on Knowledge Discovery and Data\nMining, 6555–65.\n\n\nLiu, Chenhui, Mo Zhao, Wei Li, and Anuj Sharma. 2018.\n“Multivariate Random Parameters Zero-Inflated Negative Binomial\nRegression for Analyzing Urban Midblock Crashes.” Analytic\nMethods in Accident Research 17: 32–46.\n\n\nLiu, Lei, Ya-Chen Tina Shih, Robert L Strawderman, Daowen Zhang, Bankole\nA Johnson, and Haitao Chai. 2019. “Statistical Analysis of\nZero-Inflated Nonnegative Continuous Data.” Statistical\nScience 34 (2): 253–79.\n\n\nMadsen, Lisa. 2009. “Maximum Likelihood Estimation of Regression\nParameters with Spatially Dependent Discrete Data.” Journal\nof Agricultural, Biological, and Environmental Statistics 14:\n375–91.\n\n\nManero, Jaume, Javier Béjar, and Ulises Cortés. 2018. “Wind Energy\nForecasting with Neural Networks: A Literature Review.”\nComputación y Sistemas 22 (4): 1085–98.\n\n\nMartin, Tara G, Brendan A Wintle, Jonathan R Rhodes, Petra M Kuhnert,\nScott A Field, Samantha J Low-Choy, Andrew J Tyre, and Hugh P\nPossingham. 2005. “Zero Tolerance Ecology: Improving Ecological\nInference by Modelling the Source of Zero Observations.”\nEcology Letters 8 (11): 1235–46.\n\n\nMathew, Jacob, and Rahim F Benekohal. 2021. “Highway-Rail Grade\nCrossings Accident Prediction Using Zero Inflated Negative Binomial and\nEmpirical Bayes Method.” Journal of Safety Research 79:\n211–36.\n\n\nMikolov, Tomáš. 2012. Statistical Language Models Based on Neural\nNetworks. Brno University of Technology.\n\n\nMills, Elizabeth Dastrup. 2013. Adjusting for Covariates in\nZero-Inflated Gamma and Zero-Inflated Log-Normal Models for\nSemicontinuous Data. The University of Iowa.\n\n\nMonleon, Vicente J, Lisa Madsen, and Lisa C Wilson. 2019. “Small\nArea Estimation of Zero-Inflated, Spatially Correlated Forest Variables\nUsing Copula Models.” Celebrating Progress, Possibilities,\nand Partnerships, 93.\n\n\nMozer, Michael C. 2013. “A Focused Backpropagation Algorithm for\nTemporal Pattern Recognition.” In Backpropagation,\n137–69. Psychology Press.\n\n\nMullahy, John. 1986. “Specification and Testing of Some Modified\nCount Data Models.” Journal of Econometrics 33 (3):\n341–65.\n\n\nMuncharaz, Javier Oliver. 2020. “Comparing Classic Time Series\nModels and the LSTM Recurrent Neural Network: An Application to s&p\n500 Stocks.” Finance, Markets and Valuation 6 (2):\n137–48.\n\n\nNeal, Radford M. 2003. “Slice Sampling.” The Annals of\nStatistics 31 (3): 705–67.\n\n\nNeelon, Brian, A James O’Malley, and Valerie A Smith. 2016a.\n“Modeling Zero-Modified Count and Semicontinuous Data in Health\nServices Research Part 1: Background and Overview.”\nStatistics in Medicine 35 (27): 5070–93.\n\n\n———. 2016b. “Modeling Zero-Modified Count and Semicontinuous Data\nin Health Services Research Part 2: Case Studies.” Statistics\nin Medicine 35 (27): 5094–5112.\n\n\nNeelon, Brian, Li Zhu, and Sara E Benjamin Neelon. 2015. “Bayesian\nTwo-Part Spatial Models for Semicontinuous Data with Application to\nEmergency Department Expenditures.” Biostatistics 16\n(3): 465–79.\n\n\nOlah, Christopher. 2015. “Understanding LSTM Networks.” https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nParamasivan, Senthil Kumar. 2021. “Deep Learning Based Recurrent\nNeural Networks to Enhance the Performance of Wind Energy Forecasting: A\nReview.” Revue d’Intelligence Artificielle 35 (1).\n\n\nPirani, Muskaan, Paurav Thakkar, Pranay Jivrani, Mohammed Husain Bohara,\nand Dweepna Garg. 2022. “A Comparative Analysis of ARIMA, GRU,\nLSTM and BiLSTM on Financial Time Series Forecasting.” In\n2022 IEEE International Conference on Distributed Computing and\nElectrical Circuits and Electronics (ICDCECE), 1–6. IEEE.\n\n\nPizer, Steven D, and Julia C Prentice. 2011. “Time Is Money:\nOutpatient Waiting Times and Health Insurance Choices of Elderly\nVeterans in the United States.” Journal of Health\nEconomics 30 (4): 626–36.\n\n\nPlanas, Christophe, and Alessandro Rossi. 2024. “The Slice Sampler\nand Centrally Symmetric Distributions.” Monte Carlo Methods\nand Applications 30 (3): 299–313.\n\n\nRaftery, Adrian. 1985a. “A Model for High-Order Markov\nChains.” Journal of the Royal Statistical Society Series B:\nStatistical Methodology 47 (3): 528–39.\n\n\n———. 1985b. “A New Model for Discrete-Valued Time Series:\nAutocorrelations and Extensions.” Rassegna Di Metodi\nStatistici Ed Applicazioni 3 (4): 149–62.\n\n\n———. 1994. “Change Point and Change Curve Modeling in Stochastic\nProcesses and Spatial Statistics.” Journal of Applied\nStatistical Science 1 (4): 403–23.\n\n\nRasul, Kashif, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, Rishika\nBhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, et al.\n2024. “Lag-Llama: Towards Foundation Models for Probabilistic Time\nSeries Forecasting.” https://arxiv.org/abs/2310.08278.\n\n\nRobinson, Anthony J, and Frank Fallside. 1987. The Utility Driven\nDynamic Error Propagation Network. Vol. 11. University of Cambridge\nDepartment of Engineering Cambridge.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36.\n\n\nSalman, Afan Galih, Yaya Heryadi, Edi Abdurahman, and Wayan Suparta.\n2018. “Single Layer & Multi-Layer Long Short-Term Memory\n(LSTM) Model with Intermediate Variables for Weather\nForecasting.” Procedia Computer Science 135: 89–98.\n\n\nSandhu, KS, Anil Ramachandran Nair, et al. 2019. “A Comparative\nStudy of ARIMA and RNN for Short Term Wind Speed Forecasting.” In\n2019 10th International Conference on Computing, Communication and\nNetworking Technologies (ICCCNT), 1–7. IEEE.\n\n\nSherstinsky, Alex. 2020. “Fundamentals of Recurrent Neural Network\n(RNN) and Long Short-Term Memory (LSTM) Network.” Physica D:\nNonlinear Phenomena 404: 132306.\n\n\nShi, Peng, and Lu Yang. 2018. “Pair Copula Constructions for\nInsurance Experience Rating.” Journal of the American\nStatistical Association 113 (521): 122–33.\n\n\nSiami-Namini, Sima, Neda Tavakoli, and Akbar Siami Namin. 2019. “A\nComparative Analysis of Forecasting Financial Time Series Using Arima,\nLstm, and Bilstm.” arXiv Preprint arXiv:1911.09512.\n\n\nSimmachan, T, and P Boonkrong. 2024. “A Comparison of Count and\nZero-Inflated Regression Models for Predicting Claim Frequencies in Thai\nAutomobile Insurance.” Lobachevskii Journal of\nMathematics 45 (12): 6400–6414.\n\n\nSklar, M. 1959. “Fonctions de répartition à n Dimensions Et Leurs Marges.” In\nAnnales de l’ISUP, 8:229–31. 3.\n\n\nSlime, Mekdad, Abdellah Ould Khal, Abdelhak Zoglat, Mohammed El Kamli,\nand Brahim Batti. 2025. “Optimizing Automobile Insurance Pricing:\nA Generalized Linear Model Approach to Claim Frequency and\nSeverity.” Statistics, Optimization & Information\nComputing.\n\n\nSun, Nick. 2020. “Comparison of Gaussian Copula and Random Forests\nin Zero-Inflated Spatial Prediction for Forestry Applications.”\n\n\nSutskever, Ilya, Oriol Vinyals, and Quoc V Le. 2014. “Sequence to\nSequence Learning with Neural Networks.” Advances in Neural\nInformation Processing Systems 27.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Advances in Neural\nInformation Processing Systems 30.\n\n\nWang, Peipei, Xinqi Zheng, Gang Ai, Dongya Liu, and Bangren Zhu. 2020.\n“Time Series Prediction for the Epidemic Trends of COVID-19 Using\nthe Improved LSTM Deep Learning Method: Case Studies in Russia, Peru and\nIran.” Chaos, Solitons & Fractals 140: 110214.\n\n\nWerbos, P. J. 1990. “Backpropagation Through Time: What It Does\nand How to Do It.” Proceedings of the IEEE 78 (10):\n1550–60. https://doi.org/10.1109/5.58337.\n\n\nWerbos, Paul J. 1988. “Generalization of Backpropagation with\nApplication to a Recurrent Gas Market Model.” Neural\nNetworks 1 (4): 339–56.\n\n\nWoo, Gerald, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese,\nand Doyen Sahoo. 2024. “Unified Training of Universal Time Series\nForecasting Transformers.” https://arxiv.org/abs/2402.02592.\n\n\nYang, Lu. 2022. “Nonparametric Copula Estimation for Mixed\nInsurance Claim Data.” Journal of Business & Economic\nStatistics 40 (2): 537–46.\n\n\nYang, Ming, Joseph E Cavanaugh, and Gideon KD Zamba. 2015.\n“State-Space Models for Count Time Series with Excess\nZeros.” Statistical Modelling 15 (1): 70–90.\n\n\nYoung, Derek S, Eric S Roemmele, and Xuan Shi. 2022.\n“Zero-Inflated Modeling Part II: Zero-Inflated Models for Complex\nData Structures.” Wiley Interdisciplinary Reviews:\nComputational Statistics 14 (2): e1540.\n\n\nYoung, Derek S, Eric S Roemmele, and Peng Yeh. 2022.\n“Zero-Inflated Modeling Part i: Traditional Zero-Inflated Count\nRegression Models, Their Applications, and Computational Tools.”\nWiley Interdisciplinary Reviews: Computational Statistics 14\n(1): e1541.\n\n\nYu, Yong, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. 2019. “A\nReview of Recurrent Neural Networks: LSTM Cells and Network\nArchitectures.” Neural Computation 31 (7): 1235–70.\n\n\nYu, Yunjun, Junfei Cao, and Jianyong Zhu. 2019. “An LSTM\nShort-Term Solar Irradiance Forecasting Under Complicated Weather\nConditions.” IEEE Access 7: 145651–66.\n\n\nZhang, Pengcheng, David Pitt, and Xueyuan Wu. 2022. “A New\nMultivariate Zero-Inflated Hurdle Model with Applications in Automobile\nInsurance.” ASTIN Bulletin: The Journal of the IAA 52\n(2): 393–416.\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On\nConstruction and Estimation of Stationary Mixture Transition\nDistribution Models.” Journal of Computational and Graphical\nStatistics 31 (1): 283–93.\n\n\n———. 2023a. “Bayesian Geostatistical Modeling for Discrete-Valued\nProcesses.” Environmetrics 34 (7): e2805.\n\n\n———. 2023b. “Nearest-Neighbor Mixture Models for Non-Gaussian\nSpatial Processes.” Bayesian Analysis 18 (4): 1191–1222.\n\n\nZhou, Xiaoxiao, Kai Kang, and Xinyuan Song. 2020. “Two-Part Hidden\nMarkov Models for Semicontinuous Longitudinal Data with Nonignorable\nMissing Covariates.” Statistics in Medicine 39 (13):\n1801–16.\n\n\nZou, Yixuan, and Derek S Young. 2024. “Fiducial-Based Statistical\nIntervals for Zero-Inflated Gamma Data.” Journal of\nStatistical Theory and Practice 18 (1): 12.",
    "crumbs": [
      "Bibliography",
      "Bibliography"
    ]
  },
  {
    "objectID": "appendix/chapter2/add_dzig_pzig.html",
    "href": "appendix/chapter2/add_dzig_pzig.html",
    "title": "Appendix A — PDF and CDF Plots for Zero-Inflated Gamma MTD Models",
    "section": "",
    "text": "Figure A.1: (a), (b): \\(ZIGamma(\\mu = 7, \\beta = 1, P = 0.1, \\epsilon = 0.1, 0.4)\\); (c), (d): \\(ZIGamma(\\mu = 7, \\beta = 1, P = 0.5, \\epsilon = 0.1, 0.4)\\); (e), (f): \\(ZIGamma(\\mu = 7, \\beta = 1, P = 0.7, \\epsilon = 0.1, 0.4)\\). (Left) Probability density function (PDF) and (Right) cumulative distribution function (CDF) of the zero-inflated gamma distribution with varying parameters.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>PDF and CDF Plots for Zero-Inflated Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "appendix/chapter1/add_simu.html",
    "href": "appendix/chapter1/add_simu.html",
    "title": "Appendix B — Simulations for Gamma MTD Models",
    "section": "",
    "text": "B.1 Simulation Results\nThis is chapter 1’s additional simulations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulations for Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "appendix/chapter1/add_simu.html#simulation-results",
    "href": "appendix/chapter1/add_simu.html#simulation-results",
    "title": "Appendix B — Simulations for Gamma MTD Models",
    "section": "",
    "text": "B.1.1 Convergence Diagnostics\n\nB.1.1.1 Gelman-Rubin and ACF Plots\n\n\n\n\n\n\nFigure B.1: (Left) Gelman-Rubin and (Right) ACF plot for Scenario 1’s \\(w\\).\n\n\n\n\n\n\n\n\n\nFigure B.2: (Left) Gelman-Rubin and (Right) ACF plot for Scenario 1’s \\(\\rho\\).\n\n\n\n\n\n\n\n\n\nFigure B.3: (Left) Gelman-Rubin and (Right) ACF plot for Scenario 1’s \\(\\alpha, \\beta\\).\n\n\n\n\n\nB.1.1.2 Trace and Density Plots\n\n\n\n\n\n\nFigure B.4: (Left) Trace and (Right) density plot for Scenario 1’s \\(w\\).\n\n\n\n\n\n\n\n\n\nFigure B.5: (Left) Trace and (Right) density plot for Scenario 1’s \\(\\rho\\).\n\n\n\n\n\n\n\n\n\nFigure B.6: (Left) Trace and (Right) density plot for Scenario 1’s \\(\\alpha, \\beta\\).\n\n\n\n\n\n\nB.1.2 Weight and Dependence Parameters for Copula\n\n\nB.1.3 Parameters for Marginal Distribution\n\n\n\n\n\n\nFigure B.7: Results for Scenario 3-6. Grey bars are histogram of the data. Circles are the true gamma density evaluated at the support, i.e., \\(x &gt; 0\\). Solid lines are the posterior means. Dashed lines are \\(95\\%\\) credible intervals.\n\n\n\n\n\n\n\n\n\nFigure B.8: Results for Scenario 7-9. Grey bars are histogram of the data. Circles are the true gamma density evaluated at the support, i.e., \\(x &gt; 0\\). Solid lines are the posterior means. Dashed lines are \\(95\\%\\) credible intervals.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulations for Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "appendix/chapter2/add_simu.html",
    "href": "appendix/chapter2/add_simu.html",
    "title": "Appendix C — Simulations for Zero-Inflated Gamma MTD Models",
    "section": "",
    "text": "C.1 Simulation Results\nThis is chapter 2’s additional simulations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Simulations for Zero-Inflated Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "appendix/chapter2/add_simu.html#simulation-results",
    "href": "appendix/chapter2/add_simu.html#simulation-results",
    "title": "Appendix C — Simulations for Zero-Inflated Gamma MTD Models",
    "section": "",
    "text": "C.1.1 Convergence Diagnostics\n\nC.1.1.1 Gelman-Rubin and ACF Plots\n\n\n\n\n\n\nFigure C.1: (Left) Gelman-Rubin and (Right) ACF plot for Scenario 1’s \\(w\\).\n\n\n\n\n\n\n\n\n\nFigure C.2: (Left) Gelman-Rubin and (Right) ACF plot for Scenario 1’s \\(\\rho\\).\n\n\n\n\n\n\n\n\n\nFigure C.3: (Left) Gelman-Rubin and (Right) ACF plot for Scenario 1’s \\(\\mu, \\beta, P, \\epsilon\\).\n\n\n\n\n\nC.1.1.2 Trace and Density Plots\n\n\n\n\n\n\nFigure C.4: (Left) Trace and (Right) density plot for Scenario 1’s \\(w\\).\n\n\n\n\n\n\n\n\n\nFigure C.5: (Left) Trace and (Right) density plot for Scenario 1’s \\(\\rho\\).\n\n\n\n\n\n\n\n\n\nFigure C.6: (Left) Trace and (Right) density plot for Scenario 1’s \\(\\mu, \\beta, P, \\epsilon\\).\n\n\n\n\n\n\nC.1.2 Weight and Dependence Parameters for Copula\n\n\nC.1.3 Parameters for Marginal Distribution\n\n\n\n\n\n\nFigure C.7: Simulated data for Scenario 1. Grey bars are histogram of the data. Black bar is the zero-inflated probability.\n\n\n\n\n\n\n\n\n\nFigure C.8: Results for Scenario 1. Grey bars are histogram of the data. Circles are the true gamma density evaluated at the support, i.e., \\(x &gt; 0\\). Solid lines are the posterior means. Dashed lines are \\(95\\%\\) credible intervals. Red bar is the zero-inflated probability.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Simulations for Zero-Inflated Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "appendix/chapter1/add_pred.html",
    "href": "appendix/chapter1/add_pred.html",
    "title": "Appendix D — Predictions for Gamma MTD Models",
    "section": "",
    "text": "Figure D.1: \\(95\\%\\) one-step ahead posterior predictive intervals for Gamma Scenario 3, 4, 5, 6.\n\n\n\n\n\n\n\n\n\nFigure D.2: \\(95\\%\\) one-step ahead posterior predictive intervals for Gamma Scenario 7, 8, 9.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Predictions for Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "appendix/chapter2/add_pred.html",
    "href": "appendix/chapter2/add_pred.html",
    "title": "Appendix E — Predictions for Zero-Inflated Gamma MTD Models",
    "section": "",
    "text": "Figure E.1: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 3. Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure E.2: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 4. Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure E.3: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 5. Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure E.4: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 6. Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure E.5: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 7. Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure E.6: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 8. Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure E.7: \\(95\\%\\) one-step ahead posterior predictive intervals for ZIGamma Scenario 9. Reported values show overall empirical coverage, with decomposed coverage below and above the threshold shown in parentheses: coverage (coverage for data \\(\\leq \\epsilon\\), coverage for data \\(&gt; \\epsilon\\)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Predictions for Zero-Inflated Gamma MTD Models</span>"
    ]
  },
  {
    "objectID": "appendix/chapter3/add_pred.html",
    "href": "appendix/chapter3/add_pred.html",
    "title": "Appendix F — Predictions for MTD Models vs LSTM Networks",
    "section": "",
    "text": "F.1 Gamma",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Predictions for MTD Models vs LSTM Networks</span>"
    ]
  },
  {
    "objectID": "appendix/chapter3/add_pred.html#sec-appendix-ch3-add-pred-gamma",
    "href": "appendix/chapter3/add_pred.html#sec-appendix-ch3-add-pred-gamma",
    "title": "Appendix F — Predictions for MTD Models vs LSTM Networks",
    "section": "",
    "text": "Figure F.1: One-step ahead predicted means for Gamma Scenario 3, 4, 5, 6: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means.\n\n\n\n\n\n\n\n\n\nFigure F.2: One-step ahead predicted means for Gamma Scenario 7, 8, 9: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Predictions for MTD Models vs LSTM Networks</span>"
    ]
  },
  {
    "objectID": "appendix/chapter3/add_pred.html#sec-appendix-ch3-add-pred-zigamma",
    "href": "appendix/chapter3/add_pred.html#sec-appendix-ch3-add-pred-zigamma",
    "title": "Appendix F — Predictions for MTD Models vs LSTM Networks",
    "section": "F.2 Zero-Inflated Gamma",
    "text": "F.2 Zero-Inflated Gamma\n\n\n\n\n\n\nFigure F.3: One-step ahead predicted means for Gamma Scenario 2: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means. Reported values show overall RMSE, with decomposed RMSE below and above the threshold shown in parentheses: RMSE (RMSE for data \\(\\leq \\epsilon\\), RMSE for data \\(&gt; \\epsilon\\)).\n\n\n\n\n\n\n\n\n\nFigure F.4: Zoomed-in view of one-step ahead predicted means for ZIGamma Scenario 2: Solid (black) lines are true values. Dashed (red) lines are LSTM predicted means and dashed (blue) lines are MTD predicted means. Reported values show overall RMSE, with decomposed RMSE below and above the threshold shown in parentheses: RMSE (RMSE for data \\(\\leq \\epsilon\\), RMSE for data \\(&gt; \\epsilon\\)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Predictions for MTD Models vs LSTM Networks</span>"
    ]
  },
  {
    "objectID": "appendix/chapter3/add_pred.html#sec-appendix-ch3-add-pred-real",
    "href": "appendix/chapter3/add_pred.html#sec-appendix-ch3-add-pred-real",
    "title": "Appendix F — Predictions for MTD Models vs LSTM Networks",
    "section": "F.3 Data Applications",
    "text": "F.3 Data Applications\n\n\n\n\n\n\nFigure F.5: One-step ahead prediction errors for wind speeds (m/s) at heights of (a) 50 m, (b) 10 m, and (c) 2 m above ground level: Dashed (red) lines show differences between LSTM predicted means and observed values and dashed (blue) lines show differences between MTD predicted means and observed values.\n\n\n\n\n\n\n\n\n\nFigure F.6: Zoomed-in view of one-step ahead prediction errors for wind speeds (m/s) at heights of (a) 50 m, (b) 10 m, and (c) 2 m above ground level: Dashed (red) lines show differences between LSTM predicted means and observed values and dashed (blue) lines show differences between MTD predicted means and observed values.\n\n\n\n\n\n\n\n\n\nFigure F.7: Training and validation loss curves for the LSTM model. Columns (left to right) correspond to wind speeds at 50 m, 10 m, and 2 m above ground level, respectively. Rows (top to bottom) correspond to batch sizes 8, 16, and 32, respectively.\n\n\n\n\n\n\n\n\n\nFigure F.8: Training and validation loss curves for the LSTM model. Columns (left to right) correspond to wind speeds at 50 m, 10 m, and 2 m above ground level, respectively. Rows (top to bottom) correspond to batch sizes 64, 128, and 256, respectively.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Predictions for MTD Models vs LSTM Networks</span>"
    ]
  },
  {
    "objectID": "appendix/chapter1/add_pkag.html",
    "href": "appendix/chapter1/add_pkag.html",
    "title": "Appendix G — mtd: An R Package for Modeling Gamma Time Series",
    "section": "",
    "text": "G.1 Extension to the mtd Package\nThe mtd package by Zheng, Kottas, and Sansó (2022) includes the Gaussian, Poisson, Negative Binomial, and Lomax MTD regression models. We extend the package to include the copula-based Gamma and zero-inflated Gamma MTD models.\nThe original mtd package can be installed and loaded from GitHub:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>`mtd`: An `R` Package for Modeling Gamma Time Series</span>"
    ]
  },
  {
    "objectID": "appendix/chapter1/add_pkag.html#extension-to-the-mtd-package",
    "href": "appendix/chapter1/add_pkag.html#extension-to-the-mtd-package",
    "title": "Appendix G — mtd: An R Package for Modeling Gamma Time Series",
    "section": "",
    "text": "devtools::install_github(\"xzheng42/mtd\")\nlibrary(mtd)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>`mtd`: An `R` Package for Modeling Gamma Time Series</span>"
    ]
  },
  {
    "objectID": "appendix/chapter1/add_pkag.html#installation-of-the-extended-mtd-package",
    "href": "appendix/chapter1/add_pkag.html#installation-of-the-extended-mtd-package",
    "title": "Appendix G — mtd: An R Package for Modeling Gamma Time Series",
    "section": "G.2 Installation of the Extended mtd Package",
    "text": "G.2 Installation of the Extended mtd Package\nThe extended mtd package can be installed and loaded from GitHub:\ndevtools::install_github(\"franceslinyc/mtd\")\nlibrary(mtd)\n\n\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On Construction and Estimation of Stationary Mixture Transition Distribution Models.” Journal of Computational and Graphical Statistics 31 (1): 283–93.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>`mtd`: An `R` Package for Modeling Gamma Time Series</span>"
    ]
  },
  {
    "objectID": "appendix/chapter2/add_pkag.html",
    "href": "appendix/chapter2/add_pkag.html",
    "title": "Appendix H — mtd: An R Package for Modeling Zero-inflated Gamma Time Series",
    "section": "",
    "text": "H.1 Extension to the mtd Package\nThe mtd package by Zheng, Kottas, and Sansó (2022) includes the Gaussian, Poisson, Negative Binomial, and Lomax MTD regression models. We extend the package to include the copula-based Gamma and zero-inflated Gamma MTD models.\nThe original mtd package can be installed and loaded from GitHub:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>`mtd`: An `R` Package for Modeling Zero-inflated Gamma Time Series</span>"
    ]
  },
  {
    "objectID": "appendix/chapter2/add_pkag.html#extension-to-the-mtd-package",
    "href": "appendix/chapter2/add_pkag.html#extension-to-the-mtd-package",
    "title": "Appendix H — mtd: An R Package for Modeling Zero-inflated Gamma Time Series",
    "section": "",
    "text": "devtools::install_github(\"xzheng42/mtd\")\nlibrary(mtd)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>`mtd`: An `R` Package for Modeling Zero-inflated Gamma Time Series</span>"
    ]
  },
  {
    "objectID": "appendix/chapter2/add_pkag.html#installation-of-the-extended-mtd-package",
    "href": "appendix/chapter2/add_pkag.html#installation-of-the-extended-mtd-package",
    "title": "Appendix H — mtd: An R Package for Modeling Zero-inflated Gamma Time Series",
    "section": "H.2 Installation of the Extended mtd Package",
    "text": "H.2 Installation of the Extended mtd Package\nThe extended mtd package can be installed and loaded from GitHub:\ndevtools::install_github(\"franceslinyc/mtd\")\nlibrary(mtd)\n\n\n\n\nZheng, Xiaotian, Athanasios Kottas, and Bruno Sansó. 2022. “On Construction and Estimation of Stationary Mixture Transition Distribution Models.” Journal of Computational and Graphical Statistics 31 (1): 283–93.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>`mtd`: An `R` Package for Modeling Zero-inflated Gamma Time Series</span>"
    ]
  }
]